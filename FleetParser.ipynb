{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c5a9ef",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:150px\">FleetParser</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d39ded",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29336b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests as rq\n",
    "import re\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Iterable, Union\n",
    "import transliterate\n",
    "import itertools\n",
    "from difflib import SequenceMatcher\n",
    "from heapq import nlargest as _nlargest\n",
    "import difflib\n",
    "import shutil\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7fbc22",
   "metadata": {},
   "source": [
    "### If you start parser first time, you have to install the following library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40734f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transliterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ed239",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39551ed",
   "metadata": {},
   "source": [
    "## Headers to avoid spam block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df27f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers and cookies to work with\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36',\n",
    "          \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837c0ae",
   "metadata": {},
   "source": [
    "## Parameters of matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e99320",
   "metadata": {},
   "outputs": [],
   "source": [
    "icaos = {\"iata\": True, \"icao\": True, \"wiki1\": True, \n",
    "         \"wiki2\": False, \"wiki3\": False, \"fan\": True, \n",
    "         \"mapper\": True, \"military\": False, 'flug_iata': False, \n",
    "         'flug_icao': True, 'av': True\n",
    "        }\n",
    "\n",
    "iatas = {\"iata\": True, \"icao\": False, \"wiki1\": True, \n",
    "         \"wiki2\": False, \"wiki3\": False, \"fan\": False, \n",
    "         \"mapper\": True, \"military\": False, 'flug_iata': True, \n",
    "         'flug_icao': False, 'av': True\n",
    "        }\n",
    "database_url = {\"iata\": 'http://wiki.aviabit.ru/doku.php?id=pub:ssim._apendix_a', \n",
    "                \"icao\": 'https://www.icao.int/publications/DOC8643/Pages/Search.aspx', \n",
    "                \"wiki1\": 'https://en.wikipedia.org/wiki/List_of_aircraft_type_designators',\n",
    "                \"wiki2\": 'https://en.wikipedia.org/wiki/List_of_civil_aircraft', \n",
    "                \"wiki3\": 'https://en.wikipedia.org/wiki/List_of_aircraft_by_date_and_usage_category', \n",
    "                \"fan\": 'https://www.aviationfanatic.com/ent_list.php?ent=4', \n",
    "                \"mapper\": 'https://www.greatcirclemapper.net/en/aircrafts.html', \n",
    "                \"military\": 'https://www.militaryfactory.com/aircraft/indexMAIN.php', \n",
    "                'flug_iata': 'http://www.flugzeuginfo.net/table_accodes_iata_en.php', \n",
    "                'flug_icao': 'http://www.flugzeuginfo.net/table_accodes_en.php', \n",
    "                'av': 'https://www.avcodes.co.uk/acrtypes.asp'\n",
    "        }\n",
    "\n",
    "\n",
    "icao_lst = [key for key, value in icaos.items() if value and key != 'av']\n",
    "\n",
    "skip_list = [\n",
    "#     'wiki2', 'wiki3',  \n",
    "    'military','iata', 'wiki2', 'av']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b622c7f",
   "metadata": {},
   "source": [
    "## Some exceptions that needs to be separately indicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_values_of = ['Airbus a319-100','Airbus a318-100','Airbus a319-100lr',\n",
    "                    'Airbus a320-200','Airbus a320-231','Airbus a321-100',\n",
    "                    'Airbus a321-200','Airbus a321-200p2f','Airbus a321-p2f',\n",
    "                    'Ан', 'Ми']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48bc58c",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d96d12",
   "metadata": {},
   "source": [
    "## Gathering of initial parse objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d26066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_urls(url, table_num, col_num, return_all=False, spec_xpath=None):\n",
    "    html = rq.get(url)\n",
    "    doc = lxml.html.fromstring(html.content)\n",
    "    if return_all:\n",
    "        xpth = '//*[@id=\"mw-content-text\"]/div[1]/ul[*]/li[*]/a'\n",
    "        \n",
    "        names = doc.xpath(f'{xpth}/text()')\n",
    "        urls = doc.xpath(f'{xpth}/@href')\n",
    "        return names, ['https://en.wikipedia.org'+url for url in urls]\n",
    "    if spec_xpath:\n",
    "        names = doc.xpath(f'{spec_xpath}/text()') \n",
    "        urls = doc.xpath(f'{spec_xpath}/@href')\n",
    "        return names, ['https://en.wikipedia.org'+url for url in urls]\n",
    "    xpth = f'//*[@id=\"mw-content-text\"]/div[1]/table[{table_num}]/tbody/tr[*]/td[{col_num}]/a'\n",
    "    names = doc.xpath(f'{xpth}/text()')\n",
    "    urls = doc.xpath(f'{xpth}/@href')\n",
    "    return names, ['https://en.wikipedia.org'+url for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_companies(source='pulkovo', mode='urls', headers=headers) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse list of companies cooperating with \n",
    "    Pulkovo from the airport website.\n",
    "    Or gather companies from different \n",
    "    wiki page through specifying source parameter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source: {'pulkovo', 'us', 'europe', 'cac', 'africa'\n",
    "            , 'asia', 'northamerica', 'oceania', 'southamerica',\n",
    "            'all'}, default 'pulkovo'\n",
    "        Available sources to get info about airline companies. \n",
    "        There are number of regions available.\n",
    "        NOTE! 'cac' stands for \"Central america and the Caribbean\".\n",
    "        NOTE! You can specify only 'names' if the source=='pulkovo'\n",
    "        \n",
    "    mode: {'names', 'urls'}, default urls\n",
    "        What data to return\n",
    "        NOTE! You can specify only 'names' if the source=='pulkovo'\n",
    "    \n",
    "    headers: dict\n",
    "        Headers to use for the url request \n",
    "        (necessary for avoiding spam-block)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    if source == 'pulkovo':\n",
    "        \n",
    "        url='https://pulkovoairport.ru/passengers/destinations/airlines/#'\n",
    "        \n",
    "        response = rq.get(url, headers=headers)\n",
    "        to_parse = response.text\n",
    "        soup = BeautifulSoup(to_parse, 'html.parser')\n",
    "        companies = soup.find('ul', {'class': 'tm-air uk-accordion'}).find_all('a', {'class': 'uk-accordion-title'})\n",
    "        names = []\n",
    "        for company in companies:\n",
    "            names.append(company.text)\n",
    "        \n",
    "    elif source == 'us':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_airlines_of_the_United_States', table_num=3, col_num=1) #us\n",
    "        \n",
    "    elif source == 'europe':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Europe', table_num=1, col_num=2) #eu\n",
    "        \n",
    "    elif source == 'cac':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Central_America_and_the_Caribbean', table_num=2, col_num=2) #cac\n",
    "    \n",
    "    elif source == 'africa':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Africa', table_num=2, col_num=2) #africa\n",
    "    \n",
    "    elif source == 'asia':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Asia','',2, spec_xpath='//*[@id=\"mw-content-text\"]/div[1]/div[1]/table/tbody/tr[*]/td[2]/a')\n",
    "    \n",
    "    elif source == 'northamerica':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_North_America', table_num=2, col_num=1) #na\n",
    "    \n",
    "    elif source == 'oceania':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Oceania', table_num=1, col_num=1) #oceania\n",
    "        \n",
    "    elif source == 'southamerica':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_South_America', table_num=1, col_num=2) #asia\n",
    "    \n",
    "    elif source == 'all':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_passenger_airlines',0,0,True)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return names\n",
    "        \n",
    "    if mode == 'urls':\n",
    "        \n",
    "        return urls\n",
    "    \n",
    "    if mode == 'names':\n",
    "        return names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ecf06",
   "metadata": {},
   "source": [
    "## Algorithm for searching company's article URL by its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(company: str, mode='ru', append='+авиакомпания') -> Tuple[str]:\n",
    "    \"\"\"\n",
    "    Construct the search query to look through wikipedia.org and \n",
    "    get the title and referer url of the first most apropriate result\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company: str\n",
    "        Name of company to use for constructing a search query\n",
    "    \n",
    "    mode: str\n",
    "        Type of wikipedia library language to use for search\n",
    "\n",
    "    append: str\n",
    "        Keywords to use for constructing a search \n",
    "        query for better accuracy of received result.\n",
    "        NOTE! use + in start of a string and as a separator \n",
    "        between words in append\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    company = company.replace('«','%22').replace('»','%22').replace('Group', '').replace('Holdings', '')\n",
    "    search = '+'.join(company.split())\n",
    "    query = f'https://{mode}.wikipedia.org/w/index.php?search={search}{append}&title=Special:Search&profile=advanced&fulltext=1&ns0=1'\n",
    "    response = rq.get(query, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    href = soup.find('div', {\"class\":\"mw-search-result-heading\"}).find('a').get('href')\n",
    "    title = soup.find('div', {\"class\":\"mw-search-result-heading\"}).find('a').get('title')\n",
    "    \n",
    "    return title, href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(company: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare the result and try to get the \n",
    "    most valuable result with desired company \n",
    "    url to parse further\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company: str\n",
    "        Name of company to look through\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        ru_flag = False\n",
    "        pat = re.compile(company, re.IGNORECASE)\n",
    "        ru = re.compile(\"[а-яА-Я]\")\n",
    "        en = re.compile(\"[a-zA-Z]\")\n",
    "        ru_filter = filter(ru.match, company)\n",
    "        en_filter = filter(en.match, company)\n",
    "        if list(ru_filter):\n",
    "\n",
    "            title, href = process_query(company)\n",
    "            ru_flag = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            title, href = process_query(company, mode='en', append='+airline')\n",
    "\n",
    "        if not re.match(pat, title):\n",
    "\n",
    "            if ru_flag:\n",
    "\n",
    "                title, href = process_query(company, mode='ru', append='')\n",
    "\n",
    "            else:\n",
    "\n",
    "                title, href = process_query(company, mode='en', append='')\n",
    "\n",
    "        if ru_flag:      \n",
    "            link_ = f'https://ru.wikipedia.org{href}'\n",
    "        else:\n",
    "            link_ = f'https://en.wikipedia.org{href}'\n",
    "\n",
    "        response_url = rq.get(link_, headers=headers)\n",
    "        soup_url = BeautifulSoup(response_url.text, 'html.parser')\n",
    "        link = soup_url.find('a', {'class':'interlanguage-link-target', 'lang':'en'})\n",
    "        link = link.get('href') if link is not None else link_\n",
    "\n",
    "        return link\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ca5b1",
   "metadata": {},
   "source": [
    "# Processing and transformation of parsed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0345073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the full name of the company from the title of its wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        Wikipage of company to get the fullname from\n",
    "    \n",
    "    \"\"\"\n",
    "    response = rq.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text[:15000], 'html.parser')\\\n",
    "    .find('h1',{\"class\":\"firstHeading mw-first-heading\"}).text\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_multiindex(df):\n",
    "    def unique_list(l):\n",
    "        ulist = []\n",
    "        [ulist.append(x.strip('01234567890 []')) for x in l if x not in ulist]\n",
    "        return ulist\n",
    "    new_cols = []\n",
    "    if df.columns.nlevels > 1:\n",
    "        df.columns = df.columns.map('|'.join)\n",
    "    else:\n",
    "        df.columns = df.columns.map(''.join)\n",
    "    for col in df.columns:\n",
    "        splt = col.split('|')\n",
    "        new_cols.append(' '.join(unique_list(splt)))\n",
    "    \n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83664d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df: Iterable) -> tuple:\n",
    "    \"\"\"\n",
    "    Process columns of given DataFrame to get only necessary and apropriate columns.\n",
    "    Function primarily used to get rid of MultiIndex columns' names of some DataFrames. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    columns: pd.MultiIndex, list or column Index-like iterable object\n",
    "        Columns to put in the right shape.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    dct = {'Aircraft': [], 'In service': [], 'Orders': [], 'Total_Passengers': [], 'Notes': [], }\n",
    "    match = ('Aircraft|Aircraft type|Type|Plane Name|Тип самолёта', 'In service|Inservice|In operation|No. of aircraft|No. ofaircraft|In fleet|^Fleet$|InFleet|^Total$|Number in Operation|^Number$|^Count$|In-service|Fleet (TC/PLR)|TC list|Active|Эксплуатируется', 'Order|Orders|Заказано',  'Passengers|PassengerTotal|^Passenger|Passengers(Economy)|Passenger capacity|Seating Class|Seats', 'Notes|Additional|Примечания')\n",
    "    \n",
    "    df.columns = handle_multiindex(df)\n",
    "    \n",
    "    for pat, pattern in enumerate(match):\n",
    "\n",
    "        for num, col in enumerate(df.columns):\n",
    "\n",
    "            if re.search(re.compile(pattern, re.IGNORECASE), str(col)):\n",
    "                dct[list(dct.keys())[pat]] = dct.get(list(dct.keys())[pat], []) + [(num, col)]\n",
    "\n",
    "    to_keep = []\n",
    "    for key, values in dct.items():\n",
    "\n",
    "        if len(values)>0:\n",
    "\n",
    "            if key == 'Aircraft':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "            if key == 'In service':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "            if key == 'Orders':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "            if key == 'Total_Passengers':\n",
    "                ref = False\n",
    "                for val in values:\n",
    "                    if 'Ref.' in val[1] or 'Refs' in val[1]:\n",
    "                        ref = True\n",
    "                if ref:\n",
    "                    to_keep.append(values[-2][0])\n",
    "                else:\n",
    "                    to_keep.append(values[-1][0])    \n",
    "\n",
    "            if key == 'Notes':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "\n",
    "\n",
    "    all_idx = [i for i in range(len(df.columns))]\n",
    "    to_del = list(set(all_idx)-set(to_keep))\n",
    "    cols = df.columns[to_keep]\n",
    "    return cols, to_del, to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c43230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform given dataframe\n",
    "# WIP\n",
    "\n",
    "def transform_table(df: pd.DataFrame, company: str, url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform given DataFrame table to keep only \n",
    "    necessary info and get the apropriate shape and \n",
    "    unify to a common table form  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame to handle with\n",
    "    \n",
    "    company: str\n",
    "        Company name to use in table\n",
    "    \n",
    "    cols: list-like object, Iterable\n",
    "        Columns from tranform_columns() to use for \n",
    "        transformation of given DataFrame columns\n",
    "    \n",
    "    cols_idx: list-like object, Iterable\n",
    "        Columns indices from transorm_columns() to use \n",
    "        to drop unnecessary columns in given DataFrame\n",
    "    \n",
    "    error_trans_table: Int\n",
    "        Proxy variable used to track errors during the \n",
    "        transform of table\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    cols, to_del, to_keep = transform_columns(df)\n",
    "\n",
    "    df = df.iloc[:, to_keep].copy()\n",
    "\n",
    "    df.columns = df.columns.map(''.join)\n",
    "\n",
    "    df.columns = cols\n",
    "    add_columns(df) \n",
    "\n",
    "    df['Company'] = company\n",
    "    df['Wiki URL'] = url\n",
    "\n",
    "    df.set_index(['Company', 'Wiki URL'], inplace=True)\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace(re.compile('\\[.*?\\]'),'').replace(re.compile('[0-9a]*/'), '')\n",
    "\n",
    "    rename_df_cols(df)\n",
    "\n",
    "    try:\n",
    "        df = df[(~df.Aircraft.str.lower().str.contains('total'))&(~df.Aircraft.str.lower().str.contains('всего'))&(~df.Aircraft.str.lower().str.contains('total:'))]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#     df.set_index(['Aircraft', 'In service', 'Orders'], append=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Add missing columns to the given DataFrame \n",
    "    to get its shape to complete and apropriate form \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame to handle with\n",
    "    \n",
    "    cols: list or column Index-like iterable object \n",
    "        Columns of given DataFrame\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    add = []\n",
    "    match = ('Aircraft|Aircraft type|Type|Plane Name|Тип самолёта', 'In service|Inservice|No. of aircraft|In fleet|^Fleet$|^Total$|InFleet|No. ofaircraft|In operation|In-service|Number in Operation|TC list|^Count$|Fleet (TC/PLR)|^Number$|Active|Эксплуатируется', 'Order|Orders|Заказано',  'Passengers|Passengers(Economy)|^Passenger|PassengerTotal|Passenger capacity|Seating Class|Seats', 'Notes|Additional|Примечания')\n",
    "    required = ['AirCraft', 'In service', 'Orders', 'Total_Passengers', 'Notes']\n",
    "    for pat, pattern in enumerate(match):\n",
    "        status = False\n",
    "        \n",
    "        for num, col in enumerate(df.columns):\n",
    "            if re.search(re.compile(pattern, re.IGNORECASE), str(col)):\n",
    "                status = True\n",
    "        if not status:\n",
    "            add.append(pat)\n",
    "            \n",
    "    for item in sorted(list(set(add))):\n",
    "        length = len(df.columns)\n",
    "        if item < length:\n",
    "            df.insert(item, required[item], np.nan)\n",
    "        else:\n",
    "            df[required[item]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_df_cols(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Rename the columns of given DataFrame.\n",
    "    DataFrame should be of specific form\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame which columns to rename\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['Aircraft','In service', 'Orders', 'Total_Passengers','Notes']\n",
    "    df.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a67c27",
   "metadata": {},
   "source": [
    "# Parse engine implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_txt(url: str, headers: dict, id_value = re.compile('Флот|Fleet')) -> str:\n",
    "    \"\"\"\n",
    "    Extract text containing information about aviacompany \n",
    "    fleet from the \"Fleet\" subtitle of wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        http-like string with URL address to \n",
    "        the wiki page about the company\n",
    "        \n",
    "    headers: dict\n",
    "        headers to post on the wiki server\n",
    "        \n",
    "    id_value: re.compile(..) object, default re.compile('Флот|Fleet')\n",
    "        id_value of \"Fleet\" subtitle to find desired text\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    response = rq.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    h2_tags = soup.find_all('h2')\n",
    "\n",
    "    for index, h2_tag in enumerate(h2_tags):\n",
    "        span_tag = h2_tag.find('span', {'id': id_value})\n",
    "        if span_tag:\n",
    "            break\n",
    "\n",
    "    start_index = index\n",
    "    finish_index = start_index+1\n",
    "    start_pattern = h2_tags[start_index].find('span')\n",
    "    finish_pattern = h2_tags[finish_index].find('span')\n",
    "    start = re.search(str(start_pattern), response.text).span()[0]\n",
    "    finish = re.search(str(finish_pattern), response.text).span()[1]\n",
    "    text = response.text[start:finish]\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table(text: str, url: str, header=None, match='.+') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract pd.DataFrame table from the given html-like string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        HTML string to get table from, \n",
    "        consisting of <table>, <th>, <td> tags \n",
    "    \n",
    "    url: str\n",
    "        http-like string with URL address to \n",
    "        the article about the company\n",
    "        \n",
    "    header: int, default None\n",
    "        row index to specify header row and \n",
    "        start table\n",
    "        \n",
    "    match: str, or re.compile object, default '.+'\n",
    "        pattern to specify table search\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if 'ru.wikipedia.org' in url:\n",
    "        df = pd.read_html(text, header=0, match=match)\n",
    "    else:\n",
    "        df = pd.read_html(text, header=header, match=match)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_table(url: str, company: str, headers: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Extract no-table elements with information \n",
    "    about fleet and convert it to specific form DataFrame, \n",
    "    containing complete information about some \n",
    "    company fleet from its Wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        http-like string with URL adress to \n",
    "        the article about the company\n",
    "        \n",
    "    company: str\n",
    "        name of the company to use for \n",
    "        forming DataFrame\n",
    "        \n",
    "    headers: dict\n",
    "        headers to post on the wiki server\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    global success, ind_spec\n",
    "    try:\n",
    "        \n",
    "        info = []\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        text = extract_txt(url, headers)\n",
    "\n",
    "        soup = BeautifulSoup(text)\n",
    "\n",
    "        if soup.find('li') and soup.find('ul'):\n",
    "\n",
    "            soup_ul = BeautifulSoup(text).find_all('ul')\n",
    "\n",
    "            for ul in soup_ul:\n",
    "                for li in ul.find_all('li'):\n",
    "                    info.append(li.text)\n",
    "\n",
    "        else:\n",
    "\n",
    "            info.append(soup.text.replace('\\n', ' '))\n",
    "\n",
    "        df['Parsed info'] = info\n",
    "        df['Company'] = company\n",
    "        df['Index'] = range(1, len(df)+1)\n",
    "        df.set_index(['Company', 'Index'], inplace=True)\n",
    "\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].replace(re.compile('\\[.*?\\]'),'').replace(re.compile('[0-9]*/'), '')\n",
    "        \n",
    "        got = True\n",
    "        ind_spec += 1\n",
    "        success += 1\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        df = None\n",
    "        got = False\n",
    "        \n",
    "    return df, got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4908e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(url: str, headers: dict, extra_page=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the table from company's wiki page url\n",
    "    Function firstly extracts text of subtitle 'Fleet'\n",
    "    through extract_txt(..) then extracts table from \n",
    "    the received text with extract_table(...)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        http-like string with URL address to \n",
    "        the article about the company\n",
    "        \n",
    "    headers: dict\n",
    "        headers to post on the wiki server\n",
    "        \n",
    "    extra_page: bool, default False\n",
    "        flag, used to rerun function with searching \n",
    "        additional fleet url and specifying different \n",
    "        id_value parameter for extract_txt(..)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    global success, ind\n",
    "    try:\n",
    "        \n",
    "        if not extra_page:\n",
    "\n",
    "            text = extract_txt(url, headers, id_value=re.compile('Флот|Fleet|Operators_and_fleet|Current_fleet'))\n",
    "            table = extract_table(text, url)\n",
    "            ind += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            text = extract_txt(url, headers)\n",
    "            extra_soup = BeautifulSoup(text, 'html.parser')\n",
    "            href = extra_soup.find('div', {\"class\":\"hatnote navigation-not-searchable\"}).find('a').get('href')\n",
    "            new_url = f'https://en.wikipedia.org{href}'\n",
    "            text = extract_txt(new_url, headers, id_value = re.compile('Current'))\n",
    "            table = extract_table(text, new_url)\n",
    "            ind += 1\n",
    "        \n",
    "        got = True\n",
    "        success += 1\n",
    "    \n",
    "    except:\n",
    "            \n",
    "        table = None\n",
    "        got = False\n",
    "        \n",
    "    return table, got"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e930d",
   "metadata": {},
   "source": [
    "## Logging the parse process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ea5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(company: str, num: str, url: str, urls: list, index:int, special=False) -> int:\n",
    "    \"\"\"\n",
    "    Log the tray of parser's work state\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company: str\n",
    "        name of the company to use in log\n",
    "        \n",
    "    url: str\n",
    "        http-like string with URL adress to \n",
    "        the article about the company\n",
    "    \n",
    "    urls: list\n",
    "        list of all urls of companies that \n",
    "        need to be parsed\n",
    "        \n",
    "    index: int\n",
    "        index value to use in log\n",
    "        \n",
    "    special: bool, default True\n",
    "        flag, indicating if the logging going \n",
    "        on for no-table form  fleet info wiki page \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    append = ''\n",
    "    print(*['-' for i in range(20)])\n",
    "    print(company)\n",
    "    if special:\n",
    "        append = ' special'\n",
    "        \n",
    "    print(f'URL:{url}\\nIndex{append}: {index}')\n",
    "    print(f'{num+1}/{len(urls)} URLs parsed')\n",
    "    \n",
    "    return num+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae5e4b",
   "metadata": {},
   "source": [
    "## Parse exceptions handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exception_handler(df: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Work with the occuring exceptions of \n",
    "    obtained tables from some company wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: list\n",
    "        list containing possible desired DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    na = pd.DataFrame()\n",
    "    df1 = df[0] if len(df) > 0 else na\n",
    "    try:    \n",
    "        if 0 in df1.columns or 1 in df1.columns:\n",
    "            for i in range(len(df)):\n",
    "                df1 = df[i] if len(df) > 0 else na\n",
    "                if 0 not in df1.columns or 1 not in df1.columns:\n",
    "                    break\n",
    "            else:\n",
    "                df1 = df[0]\n",
    "                df1 = df1.rename(columns=df1.iloc[0]).drop(df1.index[0])\n",
    "    except:\n",
    "        \n",
    "        df1 = na\n",
    "        \n",
    "    if df1.empty:\n",
    "        \n",
    "        df1 = na\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80172957",
   "metadata": {},
   "source": [
    "## Separate function for parsing through one URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37db1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url: str, urls: list, headers=headers) -> Union[pd.DataFrame, None]:\n",
    "        \n",
    "        global fleet, exceptions, corrupted, num\n",
    "\n",
    "        table = []\n",
    "        company = get_company_name(url)\n",
    "        \n",
    "        table, status_received = get_table(url, headers)\n",
    "\n",
    "        if not status_received:\n",
    "\n",
    "            table, status_received = get_table(url, headers, extra_page=True)\n",
    "\n",
    "        if not status_received:\n",
    "\n",
    "            notable, status_received = get_no_table(url, company, headers)\n",
    "            num = logging(company, num, url, urls, ind_spec, special=True)\n",
    "            \n",
    "            if status_received: \n",
    "                exceptions.append(notable);\n",
    "\n",
    "                return notable\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"Not able to parse data\")\n",
    "                corrupted.append(url)\n",
    "\n",
    "                return None\n",
    "\n",
    "        df1 = exception_handler(table)\n",
    "\n",
    "        df1 = transform_table(df1, company, url)\n",
    "\n",
    "        fleet.append(df1)   \n",
    "        num = logging(company, num, url, urls, ind)\n",
    "        \n",
    "        return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387ef11",
   "metadata": {},
   "source": [
    "## Separate function for parsing through one company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fleet(company: str, urls: list, headers: dict) -> Union[pd.DataFrame, None]:\n",
    "    url = get_url(company)\n",
    "    dataframe = parse(url, urls, headers)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ed19f",
   "metadata": {},
   "source": [
    "## Setting parse objects parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46649e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_urls(parse_obj=[], mode='urls'):\n",
    "    \n",
    "    if mode == 'urls':\n",
    "        print(len(parse_obj))\n",
    "        return parse_obj\n",
    "    \n",
    "    if mode == 'names':\n",
    "        print(len(parse_obj))\n",
    "        urls = [get_url(company) for company in tqdm.tqdm(parse_obj)]\n",
    "        return urls\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaf5042",
   "metadata": {},
   "source": [
    "## Concatenation of gathered tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e648f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tables(fleet: list, exceptions: list) -> Tuple[list,list]:\n",
    "    \"\"\"\n",
    "    Concat received parsed and transformed \n",
    "    tables to get the full overview of aviacompanies' \n",
    "    fleet information\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fleet: list\n",
    "        list of special-form DataFrames with \n",
    "        data about aviacompanies' fleet\n",
    "        \n",
    "    exceptions: list\n",
    "        list of special-form DataFrames with \n",
    "        data about aviacompanies' fleet,\n",
    "        that contains information from no-table \n",
    "        aviacompanies' wiki pages\n",
    "        \n",
    "    \"\"\"\n",
    "    overall = pd.concat(fleet) if fleet else None\n",
    "    overall_exp = pd.concat(exceptions) if exceptions else None\n",
    "    \n",
    "    return overall, overall_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e622317",
   "metadata": {},
   "source": [
    "## Loaders and handlers of external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0fd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_icao():\n",
    "    headers = {\n",
    "        'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        'Accept-Language': 'en,ru-RU;q=0.9,ru;q=0.8,en-US;q=0.7',\n",
    "        'Connection': 'keep-alive',\n",
    "        # 'Content-Length': '0',\n",
    "        'Origin': 'https://www.icao.int',\n",
    "        'Referer': 'https://www.icao.int/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-site',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\".Not/A)Brand\";v=\"99\", \"Google Chrome\";v=\"103\", \"Chromium\";v=\"103\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response = rq.post('https://www4.icao.int/doc8643/External/AircraftTypes', headers=headers)\n",
    "    data = response.json()\n",
    "    icao_codes = pd.DataFrame(data)\n",
    "    icao_codes.ManufacturerCode = icao_codes.ManufacturerCode.str.replace(' \\([1-9]*\\)', '', regex=True)\n",
    "    icao_codes['FullName'] = icao_codes.ManufacturerCode.str.lower().str.capitalize() +\" \" + icao_codes.ModelFullName\n",
    "    # a.columns[[9,5,0,4,1,6,7,8,2,3]]\n",
    "    icao_codes = icao_codes[['FullName', 'ManufacturerCode', 'ModelFullName', 'Designator',\n",
    "           'Description', 'AircraftDescription', 'EngineCount', 'EngineType',\n",
    "           'WTC', 'WTG']]\n",
    "    icao_codes = icao_codes.rename(columns={'Designator':'ICAO'})\n",
    "    icao_codes.FullName = icao_codes.FullName.str.lower().str.capitalize()\n",
    "    return icao_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3682b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapper(preloaded=False):\n",
    "    if preloaded:\n",
    "        return pd.read_excel('mapper_codes.xlsx')\n",
    "    mapper_codes = pd.read_html('https://www.greatcirclemapper.net/en/aircrafts.html')[0]\n",
    "    mapper_codes['FullName'] = mapper_codes.Manufacturer + ' ' + mapper_codes.Name\n",
    "    mapper_codes.Manufacturer = mapper_codes.Manufacturer.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    mapper_codes.Name = mapper_codes.Name.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    mapper_codes = mapper_codes[['Manufacturer', 'Name', 'IATA', 'Type']]\n",
    "    manufacturers = mapper_codes.Manufacturer.tolist()\n",
    "    names = mapper_codes.Name.tolist()\n",
    "    mapper_codes = mapper_codes.fillna('-')\n",
    "    resp = rq.get('https://www.greatcirclemapper.net/en/aircrafts.html')\n",
    "    sp = BeautifulSoup(resp.text, 'html.parser')\n",
    "    crafts_pages = []\n",
    "    for craft in sp.find('table').find_all('strong'):\n",
    "        if craft.find('a'):\n",
    "            craft_page = craft.find('a').get('href')\n",
    "            craft_page = f'https://www.greatcirclemapper.net/{craft_page}'\n",
    "            crafts_pages.append(craft_page)\n",
    "        else:\n",
    "            crafts_pages.append(None)\n",
    "    icaos_mapper = []\n",
    "    for link in tqdm.tqdm(crafts_pages):\n",
    "        if link:\n",
    "            response = rq.get(link)\n",
    "            soup_obj = BeautifulSoup(response.text, 'html.parser')\n",
    "            icao_mapper = soup_obj.find('dt', string='ICAO').find_next_sibling().text.strip()\n",
    "            icaos_mapper.append(icao_mapper)\n",
    "        else:\n",
    "            icaos_mapper.append(None)\n",
    "    mapper_codes['ICAO'] = icaos_mapper\n",
    "    \n",
    "    pat_split = ' / |, |/'\n",
    "    frames = []\n",
    "    for ind, obj in enumerate(manufacturers):\n",
    "        manufacturers[ind] = re.split(pat_split, obj)\n",
    "\n",
    "    mapper_codes['Manufacturer'] = manufacturers\n",
    "\n",
    "    for ind, obj in enumerate(names):\n",
    "        names[ind] = re.split(pat_split, obj)\n",
    "\n",
    "    mapper_codes['Name'] = names\n",
    "    mapper_codes.IATA = mapper_codes.IATA.str.split('      ')\n",
    "    mapper_codes.Type = mapper_codes.Type.str.split('      ')\n",
    "    mapper_codes.ICAO = mapper_codes.ICAO.str.split('      ')\n",
    "    for row in range(len(mapper_codes)):\n",
    "        for element in itertools.product(*mapper_codes.values[row]):\n",
    "            frames.append(pd.DataFrame([element], columns = mapper_codes.columns))\n",
    "\n",
    "    mapper_codes = pd.concat(frames)\n",
    "    mapper_codes.Manufacturer = mapper_codes.Manufacturer.str.replace(' Aircraft', ' ')\n",
    "    mapper_codes['FullName'] = mapper_codes.Manufacturer + ' ' + mapper_codes.Name\n",
    "    mapper_codes = mapper_codes.drop_duplicates()\n",
    "    mapper_codes = mapper_codes[['FullName', 'Manufacturer', 'Name','ICAO', 'IATA', 'Type']]\n",
    "    mapper_codes.FullName = mapper_codes.FullName.str.lower().str.capitalize()\n",
    "    mapper_codes = mapper_codes[~mapper_codes['FullName'].duplicated()]\n",
    "    mapper_codes.to_excel('mapper_codes.xlsx', index=False)\n",
    "    return mapper_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6974615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki1():\n",
    "    wiki_codes1 = pd.read_html('https://en.wikipedia.org/wiki/List_of_aircraft_type_designators')[0]\n",
    "    wiki_codes1.columns = ['ICAO', 'IATA', 'FullName']\n",
    "    wiki_codes1 = wiki_codes1[['FullName', 'ICAO', 'IATA']]\n",
    "    wiki_codes1.FullName = wiki_codes1.FullName.str.lower().str.capitalize()\n",
    "    wiki_codes1 = wiki_codes1[~wiki_codes1['FullName'].duplicated()]\n",
    "    return wiki_codes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58120cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki2():\n",
    "    resp2 = rq.get('https://en.wikipedia.org/wiki/List_of_civil_aircraft')\n",
    "    sp2 = BeautifulSoup(resp2.text, 'html.parser')\n",
    "    wiki_codes2 = []\n",
    "    for i in sp2.find_all('a', {'href': re.compile('/wiki/')})[2:1856]:\n",
    "        wiki_codes2.append(i.text)\n",
    "    wiki_codes2 = pd.DataFrame(wiki_codes2, columns=['FullName'])\n",
    "    wiki_codes2.FullName = wiki_codes2.FullName.str.lower().str.capitalize()\n",
    "    wiki_codes2 = wiki_codes2[~wiki_codes2['FullName'].duplicated()]\n",
    "    return wiki_codes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632800f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki3():\n",
    "    resp3 = rq.get('https://en.wikipedia.org/wiki/List_of_aircraft_by_date_and_usage_category')\n",
    "    sp3 = BeautifulSoup(resp3.text, 'html.parser')\n",
    "    wiki_codes3 = []\n",
    "    for i in sp3.find('table').find_next('table').find_next('table').find_next('table').find_next('table').find_all('a', {'href': re.compile('/wiki/')}):\n",
    "        wiki_codes3.append(i.text)\n",
    "    wiki_codes3 = pd.DataFrame(wiki_codes3, columns=['FullName'])\n",
    "    wiki_codes3.FullName = wiki_codes3.FullName.str.lower().str.capitalize()\n",
    "    wiki_codes3 = wiki_codes3[~wiki_codes3['FullName'].duplicated()]\n",
    "    return wiki_codes3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fan(preloaded=False):\n",
    "    if preloaded:\n",
    "        return pd.read_excel('fan_codes.xlsx')\n",
    "    tables = []\n",
    "    for page in range(1, 275):\n",
    "        tables.append(pd.read_html(f'https://www.aviationfanatic.com/ent_list.php?ent=4&pg={page}')[0][:-2])\n",
    "    fan_codes = pd.concat(tables)\n",
    "    fan_codes.columns = ['#', 'ID', 'Manufacturer', 'FullName',\n",
    "           '# of related pictures', 'ICAO',\n",
    "           'Manufacturer country', 'Category', 'Role', 'Engine type', 'Engines',\n",
    "           'WTC', 'Seats', 'First flight', 'Last manufactured', 'Total # built',\n",
    "           'Info (external)', '# of aircraft in DB', '# of related collections',\n",
    "           '# of related user comments', 'Unnamed: 20']\n",
    "    fan_codes = fan_codes[['ID', 'Manufacturer', 'FullName', 'ICAO',\n",
    "           'Manufacturer country', 'Category', 'Role', 'Engine type', 'Engines','WTC']]\n",
    "    fan_codes.FullName = fan_codes.FullName.str.lower().str.capitalize()\n",
    "    fan_codes = fan_codes[~fan_codes['FullName'].duplicated()]\n",
    "    fan_codes.to_excel('fan_codes.xlsx', index=False)\n",
    "    return fan_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3536a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mil():\n",
    "    rspn = rq.get(\"https://www.militaryfactory.com/aircraft/indexMAIN.php\").text\n",
    "    sp = BeautifulSoup(rspn, 'html.parser')\n",
    "    mil_codes = []\n",
    "    for i in sp.find_all('span', {\"class\":\"textLarge textBold textDkGray\"}):\n",
    "        mil_codes.append(i.text)\n",
    "    mil_codes = pd.Series(mil_codes).replace(' \\((.*?)\\)', '', regex=True).str.strip().tolist()\n",
    "    mil_codes = pd.DataFrame(mil_codes, columns=['FullName'])\n",
    "    mil_codes.FullName = mil_codes.FullName.str.lower().str.capitalize()\n",
    "    mil_codes = mil_codes[~mil_codes['FullName'].duplicated()]\n",
    "    return mil_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743e1e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_ssim():\n",
    "    ssim_codes = pd.read_html('http://wiki.aviabit.ru/doku.php?id=pub:ssim._apendix_a')[0]\n",
    "    ssim_codes.columns = ['FullName', 'IATA', 'Group IATA', 'Cate', 'ICAO']\n",
    "    ssim_codes.FullName = ssim_codes.FullName.str.lower().str.capitalize()\n",
    "    ssim_codes = ssim_codes[~ssim_codes['FullName'].duplicated()]\n",
    "    return ssim_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flug_icao():\n",
    "    flug_codes = pd.concat(pd.read_html('http://www.flugzeuginfo.net/table_accodes_en.php'))\n",
    "    flug_codes = flug_codes.rename(columns={'Type/Model': 'Name', 'Wake':'WTC'})\n",
    "    flug_codes.Manufacturer = flug_codes.Manufacturer.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    flug_codes.Name = flug_codes.Name.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "\n",
    "    manufacturers = flug_codes.Manufacturer.tolist()\n",
    "    names = flug_codes.Name.tolist()\n",
    "    flug_codes = flug_codes.fillna('-')\n",
    "    pat_split = ' / |, |/'\n",
    "    frames = []\n",
    "    for ind, obj in enumerate(manufacturers):\n",
    "        manufacturers[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Manufacturer'] = manufacturers\n",
    "\n",
    "    for ind, obj in enumerate(names):\n",
    "        names[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Name'] = names\n",
    "\n",
    "    flug_codes.ICAO = flug_codes.ICAO.str.split('      ')\n",
    "    flug_codes.WTC = flug_codes.WTC.str.split('      ')\n",
    "    for row in range(len(flug_codes)):\n",
    "        for element in itertools.product(*flug_codes.values[row]):\n",
    "            frames.append(pd.DataFrame([element], columns = flug_codes.columns))\n",
    "\n",
    "    flug_codes = pd.concat(frames)\n",
    "    flug_codes['FullName'] = flug_codes.Manufacturer + ' ' + flug_codes.Name\n",
    "    flug_codes = flug_codes[['FullName', 'Manufacturer', 'Name', 'ICAO', 'WTC']]\n",
    "    flug_codes = flug_codes[~flug_codes['FullName'].duplicated()]\n",
    "    return flug_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flug_iata():\n",
    "    flug_codes = pd.read_html('http://www.flugzeuginfo.net/table_accodes_iata_en.php')[0]\n",
    "    flug_codes = flug_codes.rename(columns={'Type/Model': 'Name', 'Wake':'WTC'})\n",
    "    flug_codes.Manufacturer = flug_codes.Manufacturer.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    flug_codes.Name = flug_codes.Name.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "\n",
    "    manufacturers = flug_codes.Manufacturer.tolist()\n",
    "    names = flug_codes.Name.tolist()\n",
    "    flug_codes = flug_codes.fillna('-')\n",
    "    pat_split = ' / |, |/'\n",
    "    frames = []\n",
    "    for ind, obj in enumerate(manufacturers):\n",
    "        manufacturers[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Manufacturer'] = manufacturers\n",
    "\n",
    "    for ind, obj in enumerate(names):\n",
    "        names[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Name'] = names\n",
    "\n",
    "    flug_codes.IATA = flug_codes.IATA.str.split('      ')\n",
    "    flug_codes.WTC = flug_codes.WTC.str.split('      ')\n",
    "    for row in range(len(flug_codes)):\n",
    "        for element in itertools.product(*flug_codes.values[row]):\n",
    "            frames.append(pd.DataFrame([element], columns = flug_codes.columns))\n",
    "\n",
    "    flug_codes = pd.concat(frames)\n",
    "    flug_codes['FullName'] = flug_codes.Manufacturer + ' ' + flug_codes.Name\n",
    "    flug_codes = flug_codes[['FullName', 'Manufacturer', 'Name', 'IATA', 'WTC']]\n",
    "    flug_codes = flug_codes[~flug_codes['FullName'].duplicated()]\n",
    "    return flug_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcaa3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_av():\n",
    "    av_codes = pd.read_html('https://www.avcodes.co.uk/acrtypes.asp')[0].rename(columns={'IATACode':\"IATA\", 'ICAOCode':'ICAO', 'Manufacturer and Aircraft Type / Model': 'FullName'})\n",
    "    av_codes = av_codes[~av_codes['FullName'].duplicated()]\n",
    "    return av_codes[~av_codes['FullName'].duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18563658",
   "metadata": {},
   "source": [
    "## Separate function for loading all external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_databases():\n",
    "    ssim_codes = load_ssim()\n",
    "    icao_codes = load_icao()\n",
    "    wiki_codes1 = load_wiki1()\n",
    "    wiki_codes2 = load_wiki2()\n",
    "    wiki_codes3 = load_wiki3()\n",
    "    fan_codes = load_fan(preloaded=True)\n",
    "    mapper_codes = load_mapper(preloaded=True)\n",
    "    mil_codes = load_mil()\n",
    "    flug_icao = load_flug_icao()\n",
    "    flug_iata = load_flug_iata()\n",
    "    av_codes = load_av()\n",
    "    all_codes = {\n",
    "             'iata':ssim_codes, 'icao':icao_codes, 'wiki1': wiki_codes1, \n",
    "             'wiki2':wiki_codes2, 'wiki3':wiki_codes3, 'fan': fan_codes, \n",
    "             'mapper': mapper_codes, 'military': mil_codes, 'flug_iata': flug_iata,\n",
    "             'flug_icao':flug_icao, 'av': av_codes\n",
    "            }\n",
    "    return all_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c978e",
   "metadata": {},
   "source": [
    "## Modification difflib.get_close_matches() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f5a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_close_matches(word, possibilities, n=1, cutoff=0.6):\n",
    "\n",
    "    if not n >  0:\n",
    "        raise ValueError(\"n must be > 0: %r\" % (n,))\n",
    "    if not 0.0 <= cutoff <= 1.0:\n",
    "        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n",
    "    result = []\n",
    "    s = SequenceMatcher()\n",
    "    s.set_seq2(word)\n",
    "    for x in possibilities:\n",
    "        s.set_seq1(x)\n",
    "        if s.real_quick_ratio() >= cutoff and \\\n",
    "           s.quick_ratio() >= cutoff and \\\n",
    "           s.ratio() >= cutoff:\n",
    "            result.append((s.ratio(), x))\n",
    "\n",
    "    # Move the best scorers to head of list\n",
    "    result = _nlargest(n, result)\n",
    "    # Strip scores for the best n matches\n",
    "    return [(score, x) for score, x in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0489896",
   "metadata": {},
   "source": [
    "## Related function to get_close_matches() that returns only one match with its score (cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match(x, other, cutoff):\n",
    "    matches = get_close_matches(x, other, n=1, cutoff=cutoff)\n",
    "    return matches[0][0], matches[0][1] if matches else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813ff55",
   "metadata": {},
   "source": [
    "## Main algorithm of matching fuzzy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29748eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_merge(df1, df2, left_on, right_on, how='left', cutoff=0.65, steps=1, log_tray=True, num=0):\n",
    "    df_other= df1.copy()\n",
    "    step_trays = []\n",
    "    cuts = []\n",
    "    left = [None for i in range(len(df1[left_on]))]\n",
    "    scores = [None for i in range(len(df1[left_on]))]\n",
    "    for step in range(steps):    \n",
    "        step_tray = []\n",
    "        print(cutoff)\n",
    "        for ind, x in enumerate(df_other[left_on]):\n",
    "            try:\n",
    "                score, match = get_closest_match(x, df2[right_on][df2[right_on].str.contains(x.split()[0])], cutoff)\n",
    "            except:\n",
    "                score = None\n",
    "                match = None\n",
    "                \n",
    "            if match and not left[ind]:\n",
    "                left[ind] = match\n",
    "                scores[ind] = score\n",
    "\n",
    "            step_tray.append(match)\n",
    "        cuts.append(cutoff)\n",
    "        \n",
    "        cutoff = cutoff - 0.01\n",
    "\n",
    "        step_trays.append(step_tray)\n",
    "        \n",
    "    df1[right_on] = left\n",
    "    df1['cutoffs'] = scores\n",
    "    if log_tray:\n",
    "        return df1.merge(df2, on=right_on, how='left'), step_trays\n",
    "    return df1.merge(df2, on=right_on, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915449a8",
   "metadata": {},
   "source": [
    "## Deriving and processing column that needs to be fuzzy matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73767ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_match_col(df, change_values_of):\n",
    "    proxy = df.copy()\n",
    "    \n",
    "    proxy[\"Crafts\"] = overall.Aircraft\n",
    "    proxy_crafts = proxy.Crafts.to_frame().reset_index(drop=True)\n",
    "    proxy_crafts.Crafts = proxy_crafts.Crafts.str.lower().str.capitalize()\n",
    "    \n",
    "    for change_value in change_values_of:\n",
    "        proxy_crafts.Crafts[proxy_crafts.Crafts.str.contains(change_value, na=False)] = \\\n",
    "        proxy_crafts.Crafts[proxy_crafts.Crafts.str.contains(change_value, na=False)] \\\n",
    "        .str.replace('(-100)', '', regex=True).str.replace('-200', '').str.replace('-231','') \\\n",
    "        .str.replace('Ан', 'Антонов ан').str.replace('Ми', 'Мил ми')\n",
    "    \n",
    "    trans = lambda x: transliterate.translit(x, 'ru', reversed=True) if x and type(x)==str else np.nan\n",
    "    proxy_crafts.Crafts = proxy_crafts.Crafts.apply(trans)\n",
    "    \n",
    "    return proxy_crafts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afb807",
   "metadata": {},
   "source": [
    "## Match engine algorithm that performs analysis through all loaded databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ab4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(main_frame, analyzers, steps, cutoff=0.65):\n",
    "    \n",
    "    for num, analyzer in enumerate(analyzers):\n",
    "        start = datetime.now()\n",
    "        data = analyzer[1].copy()\n",
    "        print(f'Set lenght: {len(data)}')\n",
    "        data.FullName = data.FullName.str.lower().str.capitalize().reset_index(drop=True)\n",
    "#         working_frame = main_frame.copy()\n",
    "        main_frame = fuzzy_merge(main_frame, data, left_on='Crafts', right_on='FullName', how='left', cutoff=cutoff, steps=steps, log_tray=False, num=num)\n",
    "        main_frame.rename(columns={'FullName': f'FullName_{num}', 'cutoffs': f'cutoffs_{num}'}, inplace=True)\n",
    "        end = datetime.now()\n",
    "        print(end-start)\n",
    "        print(len(main_frame))\n",
    "    return main_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c04373",
   "metadata": {},
   "source": [
    "## Derivation of best cutoff(score) through all cutoffs for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_best_score(df, score_col_name='best_cutoff'):\n",
    "    if df.dropna(thresh=2).empty:\n",
    "        df[score_col_name] = np.nan\n",
    "        return None\n",
    "    df[score_col_name] = df[[column for column in df.columns if 'cutoffs' in column]].idxmax(axis=1)\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09d219",
   "metadata": {},
   "source": [
    "## Derivation of best matches and their original databases indices for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8046667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_best(df, best_cutoff_col):\n",
    "    if df.dropna(thresh=2).empty:\n",
    "        df['best_match'] = np.nan\n",
    "        df['best_match_index'] = np.nan\n",
    "        df['codes_base_num'] = np.nan\n",
    "        return None\n",
    "    df['best_match'] = df[best_cutoff_col][df[best_cutoff_col].str.contains('_', na=False)].replace('cutoffs', 'FullName', regex=True)\n",
    "    df['best_match_index'] = (df['best_match'].str[-1].astype(float)*2 + 1).astype(int, errors='ignore')\n",
    "    df['codes_base_num'] = df['best_match'].str[-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60db5b",
   "metadata": {},
   "source": [
    "## Derivation original databases of best matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5d9abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_db(df, db_ind_col, db_list):\n",
    "    orig_database = []\n",
    "    for row in df.index:\n",
    "        try:\n",
    "            key = db_list[int(df.loc[row, db_ind_col])][0]\n",
    "        except:\n",
    "            key = np.nan\n",
    "            \n",
    "        orig_database.append(key)\n",
    "    \n",
    "    return orig_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c8553",
   "metadata": {},
   "source": [
    "## Check for designator (ICAO, IATA) availability in original databases for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9885c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desig_availability(df, db_col, db_available):\n",
    "    desig_truth = []\n",
    "    for row in df.index:\n",
    "        dataset = df.loc[row, db_col]\n",
    "        try:\n",
    "            have_desig = db_available[dataset]\n",
    "        except:\n",
    "            have_desig = False\n",
    "            \n",
    "        desig_truth.append(have_desig)\n",
    "    \n",
    "    return desig_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da98704",
   "metadata": {},
   "source": [
    "## Derivation of designator (ICAO, IATA) values for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88800f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desig_values(df, db_col, desig_truth_col, dbs_dct, desig):\n",
    "    \n",
    "    desig_codes = []\n",
    "    \n",
    "    for row in df.index:\n",
    "    \n",
    "        dataset = df.loc[row, db_col]\n",
    "\n",
    "\n",
    "        if df.loc[row, desig_truth_col] == True:\n",
    "\n",
    "            match_col = df.loc[row, 'best_match']\n",
    "            match_value = df.loc[row, match_col]\n",
    "\n",
    "            desig_value = dbs_dct[dataset].loc[dbs_dct[dataset]['FullName']==match_value, desig]\n",
    "\n",
    "            desig_value = desig_value.item() if len(desig_value) else np.nan\n",
    "\n",
    "        else:\n",
    "\n",
    "            desig_value = np.nan\n",
    "            \n",
    "        desig_codes.append(desig_value)\n",
    "        \n",
    "    return desig_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc54f4",
   "metadata": {},
   "source": [
    "## Obtaining values (best_match/best_cutoff) for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d06d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(df, column):\n",
    "    if df.dropna(thresh=4).empty:\n",
    "        values = np.nan\n",
    "        return values\n",
    "    values = []\n",
    "    for row in df.index:\n",
    "        val_col = df.loc[row, column]\n",
    "        if val_col is not np.nan:\n",
    "            value = df.loc[row, val_col]\n",
    "        else:\n",
    "            value = np.nan\n",
    "        values.append(value)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb830fc7",
   "metadata": {},
   "source": [
    "## Rematching algorithm that tries to guess unmatched observations one more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4625003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rematching(df, dbs_dct, skip_list, icao_list, icaos, iatas):\n",
    "\n",
    "    rematch_vals = []\n",
    "    for row in df.index:\n",
    "        no_icao = not df.loc[row, 'have_icao']\n",
    "        rematch_vals.append(no_icao)\n",
    "    rematch_rows = df[rematch_vals].copy()\n",
    "    if rematch_rows.dropna(thresh=4).empty:\n",
    "        return pd.DataFrame(columns=['Crafts', 'best_cutoff', 'orig_database', 'best_match',\n",
    "                                     'best_match_index', 'codes_base_num', 'have_icao', 'have_iata',\n",
    "                                     'ICAO', 'IATA', 'researched_value', 'cutoff']\n",
    "                           )\n",
    "\n",
    "    new_matches = []\n",
    "    for row in rematch_rows.index:\n",
    "\n",
    "        new_match_col = rematch_rows.loc[row, 'best_match']\n",
    "        if new_match_col is not np.nan:\n",
    "            new_match = rematch_rows.loc[row, new_match_col]\n",
    "        else:\n",
    "            new_match = np.nan\n",
    "\n",
    "        new_matches.append(new_match)\n",
    "        \n",
    "    rematch_rows['rematch_values'] = new_matches\n",
    "    \n",
    "    rematch_old_index = rematch_rows.index\n",
    "    \n",
    "    rematch_crafts = rematch_rows.rematch_values.to_frame().rename(columns={'rematch_values':'Crafts'})\n",
    "    \n",
    "    dbs_rematch = [(key, dbs_dct[key]['FullName'].to_frame()) for key in dbs_dct \\\n",
    "                   if key not in skip_list and key in icao_list]\n",
    "\n",
    "    rematch_df = analyze(rematch_crafts, dbs_rematch, steps=1, cutoff=0.65)\n",
    "    \n",
    "    rematch_df.index = rematch_old_index\n",
    "    \n",
    "    derive_best_score(rematch_df)\n",
    "    \n",
    "    rematch_df = get_info_best(rematch_df, 'best_cutoff')\n",
    "    \n",
    "    rematch_df['orig_database'] = find_db(rematch_df, 'codes_base_num', dbs_rematch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    rematch_df['have_icao'] = desig_availability(rematch_df, 'orig_database', icaos)\n",
    "    rematch_df['have_iata'] = desig_availability(rematch_df, 'orig_database', iatas)\n",
    "    \n",
    "    rematch_df['ICAO'] = get_desig_values(rematch_df, 'orig_database', 'have_icao', dbs_dct, 'ICAO')\n",
    "    rematch_df['IATA'] = get_desig_values(rematch_df, 'orig_database', 'have_iata', dbs_dct, 'IATA')\n",
    "    \n",
    "    rematch_df['researched_value'] = get_values(rematch_df, 'best_match')\n",
    "    rematch_df['cutoff'] = get_values(rematch_df, 'best_cutoff')\n",
    "    return rematch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939bb57",
   "metadata": {},
   "source": [
    "## Matching algorithm that contains all processing and values derivation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ba6540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(main_frame, dbs_dct, skip_list, icao_list, icaos, iatas, cutoff=0.65):\n",
    "    \n",
    "    dfs = [(key, dbs_dct[key]['FullName'].to_frame()) for key in dbs_dct if key not in skip_list]\n",
    "    \n",
    "    analysis = analyze(main_frame, dfs[:], steps=1, cutoff=cutoff)\n",
    "    df = analysis.copy()\n",
    "    \n",
    "    derive_best_score(df)\n",
    "\n",
    "    \n",
    "    df = get_info_best(df, 'best_cutoff')\n",
    "\n",
    "    \n",
    "    df['orig_database'] = find_db(df, 'codes_base_num', dfs)\n",
    "\n",
    "    df['have_icao'] = desig_availability(df, 'orig_database', icaos)\n",
    "    df['have_iata'] = desig_availability(df, 'orig_database', iatas)\n",
    "    df['ICAO'] = get_desig_values(df, 'orig_database', 'have_icao', dbs_dct, 'ICAO')\n",
    "    df['IATA'] = get_desig_values(df, 'orig_database', 'have_iata', dbs_dct, 'IATA')\n",
    "    df['researched_value'] = get_values(df, 'best_match')\n",
    "    df['cutoff'] = get_values(df, 'best_cutoff')\n",
    "    \n",
    "    analysis_rematch = rematching(df, dbs_dct, skip_list, icao_list, icaos, iatas)\n",
    "    \n",
    "    for row in analysis_rematch.index:\n",
    "        df.loc[row, 'orig_database'] = analysis_rematch.loc[row, 'orig_database']\n",
    "        df.loc[row, 'ICAO'] = analysis_rematch.loc[row, 'ICAO']\n",
    "        df.loc[row, 'IATA'] = analysis_rematch.loc[row, 'IATA']\n",
    "        df.loc[row, 'researched_value'] = analysis_rematch.loc[row, 'researched_value']\n",
    "        df.loc[row, 'cutoff'] = analysis_rematch.loc[row, 'cutoff']\n",
    "    \n",
    "    for row in df.index:\n",
    "    \n",
    "        dataset = df.loc[row, 'orig_database']\n",
    "\n",
    "\n",
    "        if not pd.isnull(dataset):\n",
    "\n",
    "            df.loc[row, 'orig_database'] = database_url[dataset]\n",
    "\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "        \n",
    "    df = df[['Crafts', 'best_match', 'best_match_index', 'codes_base_num',\n",
    "             'have_icao', 'have_iata', 'ICAO', 'IATA', 'researched_value',\n",
    "             'cutoff', 'orig_database']]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5c1893",
   "metadata": {},
   "source": [
    "## Add info on received ICAO and IATA designators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9644df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df, dbs_dct):\n",
    "    fan_cols = ['Manufacturer', 'FullName', 'ICAO', 'Manufacturer country', 'Category', 'Role']\n",
    "    keep_cols = ['ICAO', 'IATA', 'ManufacturerCode', 'Description', \n",
    "                 'AircraftDescription', 'EngineCount', 'EngineType', 'WTC', 'WTG', \n",
    "                 'Role', 'researched_value', 'cutoff', 'orig_database']\n",
    "    \n",
    "    df_merged = pd.merge(df, dbs_dct['icao'].groupby(by='ICAO').first() \\\n",
    "                         .reset_index(), on='ICAO', how='left', suffixes=(None, '_check'))\n",
    "    \n",
    "    df_merged1 = pd.merge(df_merged, dbs_dct['iata'].groupby(by='IATA').first() \\\n",
    "                          .reset_index(), on='IATA', how='left', suffixes=(None, '_check'))\n",
    "    \n",
    "    df_merged2 = pd.merge(df_merged1, dbs_dct['fan'][fan_cols].groupby(by='ICAO').first() \\\n",
    "                          .reset_index(), on='ICAO', how='left')\n",
    "    \n",
    "    df_merged2 = df_merged2[keep_cols]\n",
    "    return df_merged2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8d5ef",
   "metadata": {},
   "source": [
    "## Creation of the final report files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(overall, additional_info, overall_exp, corrupted, db_path) -> None:\n",
    "    \"\"\"\n",
    "    Create Excel report about aviacompanies' fleet \n",
    "    with making directory and specifying date and time\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    overall: pd.DataFrame\n",
    "        DataFrame with information \n",
    "        about aviacompanies' fleet\n",
    "        \n",
    "    overal_exp: pd.DataFrame\n",
    "        DataFrame with information \n",
    "        about parsed wiki pages exceptions \n",
    "        of aviacompanies' fleet \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    t_date = datetime.now().strftime('%d_%m_%Y, %H.%M')\n",
    "\n",
    "    Path(f\"{path}/report_{t_date}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    os.chdir(f\"{path}/report_{t_date}\")\n",
    "    \n",
    "    multiindex_cols = ['Company', 'Wiki URL', 'Aircraft', 'In service', 'Orders']\n",
    "    \n",
    "    if corrupted: \n",
    "        pd.DataFrame(corrupted, columns=['Corrupted URLs']).to_excel(f'corrupted_URLs_{t_date}.xlsx', index=False)\n",
    "    \n",
    "    if overall is not None:\n",
    "        pd.concat([overall.reset_index(), additional_info], axis=1).set_index(multiindex_cols) \\\n",
    "        .to_excel(f'fleet_report_{t_date}.xlsx')\n",
    "    \n",
    "    if overall_exp is not None: \n",
    "        overall_exp.to_excel(f'fleet_report_exceptions_{t_date}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962fc07d",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd817a41",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d0499",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135da717",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3ce10",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d2761",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c065c",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f8d996",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:100px\">Parser</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476790a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d83767",
   "metadata": {},
   "source": [
    "### 1. Setting up of initial data and getting parse urls\n",
    "1. In a variable **source** write down what source to parse objects from\n",
    "\n",
    "    possible options: **{'pulkovo', 'us', 'europe', 'cac', 'africa', 'asia', 'northamerica', 'oceania', 'southamerica', 'all'}**\n",
    "\n",
    "\n",
    "2. In a variable **mode**  write what type of data to gather\n",
    "\n",
    "    possible options: **{'urls', 'names'}**\n",
    "\n",
    "NOTE! You can use only 'names' option in a variable **mode** if the source=='pulkovo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4234b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'pulkovo'\n",
    "mode = 'names'\n",
    "parse_obj = get_list_companies(source=source, mode=mode, headers=headers)\n",
    "urls = get_parse_urls(parse_obj=parse_obj, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50297462",
   "metadata": {},
   "source": [
    "### 2. Main fleet parser (uses urls variable to iterate and parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72894f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fleet = []\n",
    "ind = -1\n",
    "ind_spec = -1\n",
    "success = 0\n",
    "\n",
    "corrupted = []\n",
    "exceptions = []\n",
    "\n",
    "for num, url in enumerate(urls[:]):\n",
    "\n",
    "    df = []\n",
    "    company = get_company_name(url)\n",
    "    \n",
    "    df, got = get_table(url, headers)\n",
    "\n",
    "    if not got:\n",
    "        \n",
    "        df, got = get_table(url, headers, extra_page=True)\n",
    "            \n",
    "    if not got:\n",
    "        \n",
    "        df1, got = get_no_table(url, company, headers)\n",
    "        \n",
    "        num = logging(company, num, url, urls, ind_spec, special=True)\n",
    "        if got:\n",
    "            \n",
    "            exceptions.append(df1)\n",
    "            continue\n",
    "   \n",
    "    if not got:\n",
    "        \n",
    "        print(\"Not able to parse data\")\n",
    "        corrupted.append(url)\n",
    "        continue\n",
    "    \n",
    "    df1 = exception_handler(df)\n",
    "    \n",
    "    df1 = transform_table(df1, company, url)\n",
    "        \n",
    "    fleet.append(df1)   \n",
    "    num = logging(company, num, url, urls, ind)\n",
    "    \n",
    "print('', '', '', sep='\\n')\n",
    "print(f'{success}/{len(urls)} companies successfully parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b744496",
   "metadata": {},
   "source": [
    "#### Statistics for parsed urls\n",
    "* len(fleet) -- stands for the number of processed tables with fleet information\n",
    "* len(exception) -- stands for the number of received non-table fleet info that for the moment could not be processed further\n",
    "* len(corrupted) -- stands for the number of URLs that could not be parsed due to the different reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516673e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(fleet), len(exceptions), len(corrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db657237",
   "metadata": {},
   "source": [
    "#### List of all corrupted URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc821c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d57341",
   "metadata": {},
   "source": [
    "### 3. Concatenation of received parsed tables with fleet info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall, overall_exp = concat_tables(fleet, exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989051e",
   "metadata": {},
   "source": [
    "### 4. Setting up the storage and databases paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be401771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage path\n",
    "\n",
    "# path = input() # C:\\\\Users\\\\Asus\\\\Downloads\\\\parser\\\\reports\n",
    "path = 'C:\\\\Users\\\\Asus\\\\Downloads\\\\parser\\\\reports'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to databases\n",
    "\n",
    "# db_path = input() # datasets/\n",
    "db_path = 'datasets/'\n",
    "try:\n",
    "    shutil.copy(f\"{db_path}fan_codes.xlsx\", f\"{path}\")\n",
    "    shutil.copy(f\"{db_path}mapper_codes.xlsx\", f\"{path}\")\n",
    "    preloaded=True\n",
    "except:\n",
    "    preloaded=False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a8c05",
   "metadata": {},
   "source": [
    "### 5. Loading and processing external databases\n",
    "####  Any of realisations might be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# all_codes = load_databases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f59a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ssim_codes = load_ssim()\n",
    "icao_codes = load_icao()\n",
    "wiki_codes1 = load_wiki1()\n",
    "wiki_codes2 = load_wiki2()\n",
    "wiki_codes3 = load_wiki3()\n",
    "fan_codes = load_fan(preloaded=preloaded)\n",
    "mapper_codes = load_mapper(preloaded=preloaded)\n",
    "mil_codes = load_mil()\n",
    "flug_icao = load_flug_icao()\n",
    "flug_iata = load_flug_iata()\n",
    "av_codes = load_av()\n",
    "all_codes = {\n",
    "         'iata':ssim_codes, 'icao':icao_codes, 'wiki1': wiki_codes1, \n",
    "         'wiki2':wiki_codes2, 'wiki3':wiki_codes3, 'fan': fan_codes, \n",
    "         'mapper': mapper_codes, 'military': mil_codes, 'flug_iata': flug_iata,\n",
    "         'flug_icao':flug_icao, 'av': av_codes\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee027e6b",
   "metadata": {},
   "source": [
    "### 6. Derivation and processing the column on which the match will be searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_frame = derive_match_col(overall, change_values_of)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c44767",
   "metadata": {},
   "source": [
    "### 7. Advanced fuzzy matching of ICAO and IATA codes to the aircrafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b3904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matches = matching(main_frame, all_codes, skip_list, icao_lst, icaos, iatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03473c52",
   "metadata": {},
   "source": [
    "### 8. Adding the info to the matched ICAO and IATA designators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ced45",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_full = add_info(matches, all_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546348d3",
   "metadata": {},
   "source": [
    "### 9. Creation of final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c75965",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_report(overall, matches_full, overall_exp, corrupted, db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48157476",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
