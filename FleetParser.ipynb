{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510d08cb",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:150px\">FleetParser</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d6b76d",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29336b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import grequests\n",
    "import requests as rq\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Iterable, Union\n",
    "import transliterate\n",
    "import itertools\n",
    "from difflib import SequenceMatcher\n",
    "from heapq import nlargest as _nlargest\n",
    "import shutil\n",
    "import lxml.html\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1685c9",
   "metadata": {},
   "source": [
    "### If you start parser first time, install the following library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40734f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transliterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e02c03c",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39551ed",
   "metadata": {},
   "source": [
    "## Headers to avoid spam block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df27f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers and cookies to work with\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.63 Safari/537.36',\n",
    "          \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\"\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23462eee",
   "metadata": {},
   "source": [
    "## Parameters of matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "icaos_aircraft = {\"iata\": True, \"icao\": True, \"wiki1\": True, \n",
    "         \"wiki2\": False, \"wiki3\": False, \"fan\": True, \n",
    "         \"mapper\": True, \"military\": False, 'flug_iata': False, \n",
    "         'flug_icao': True, 'av': True\n",
    "        }\n",
    "\n",
    "iatas_aircraft = {\"iata\": True, \"icao\": False, \"wiki1\": True, \n",
    "         \"wiki2\": False, \"wiki3\": False, \"fan\": False, \n",
    "         \"mapper\": True, \"military\": False, 'flug_iata': True, \n",
    "         'flug_icao': False, 'av': True\n",
    "        }\n",
    "\n",
    "icaos_airlines = {\n",
    "                  \"code\":True,\n",
    "                  \"av_airlines\":True,\n",
    "                  \"flug_airlines\":True,\n",
    "                  \"aircodes\":True\n",
    "                }\n",
    "\n",
    "iatas_airlines = {\n",
    "                  \"code\":True,\n",
    "                  \"av_airlines\":True,\n",
    "                  \"flug_airlines\":True,\n",
    "                  \"aircodes\":True\n",
    "                 }\n",
    "\n",
    "database_url = {\"iata\": 'http://wiki.aviabit.ru/doku.php?id=pub:ssim._apendix_a', \n",
    "                \"icao\": 'https://www.icao.int/publications/DOC8643/Pages/Search.aspx', \n",
    "                \"wiki1\": 'https://en.wikipedia.org/wiki/List_of_aircraft_type_designators',\n",
    "                \"wiki2\": 'https://en.wikipedia.org/wiki/List_of_civil_aircraft', \n",
    "                \"wiki3\": 'https://en.wikipedia.org/wiki/List_of_aircraft_by_date_and_usage_category', \n",
    "                \"fan\": 'https://www.aviationfanatic.com/ent_list.php?ent=4', \n",
    "                \"mapper\": 'https://www.greatcirclemapper.net/en/aircrafts.html', \n",
    "                \"military\": 'https://www.militaryfactory.com/aircraft/indexMAIN.php', \n",
    "                'flug_iata': 'http://www.flugzeuginfo.net/table_accodes_iata_en.php', \n",
    "                'flug_icao': 'http://www.flugzeuginfo.net/table_accodes_en.php', \n",
    "                'av': 'https://www.avcodes.co.uk/acrtypes.asp',\n",
    "                \"code\": 'https://en.wikipedia.org/wiki/List_of_airline_codes',\n",
    "                \"av_airlines\":'https://www.avcodes.co.uk/airlcodesearch.asp',\n",
    "                \"flug_airlines\":'http://www.flugzeuginfo.net/table_airlinecodes_airline_en.php',\n",
    "                \"aircodes\":'https://airlinecodes.info/icao'\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "icao_aircraft_lst = [key for key, value in icaos_aircraft.items() if value and key != 'av']\n",
    "icao_airlines_lst = [key for key, value in icaos_airlines.items() if value]\n",
    "\n",
    "skip_list = [\n",
    "#     'wiki2', 'wiki3',  \n",
    "    'military',\n",
    "    'iata',\n",
    "#     'wiki2', \n",
    "    'av'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854914e6",
   "metadata": {},
   "source": [
    "## Some exceptions that needs to be separately indicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7aec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_values_of_aircrafts = ['Airbus a319-100','Airbus a318-100','Airbus a319-100lr',\n",
    "                    'Airbus a320-200','Airbus a320-231','Airbus a321-100',\n",
    "                    'Airbus a321-200','Airbus a321-200p2f','Airbus a321-p2f',\n",
    "                    'Ан', 'Ми']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c047d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd90d773",
   "metadata": {},
   "source": [
    "## Gathering of initial parse objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b40aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_urls(url, table_num, col_num, return_all=False, spec_xpath=None):\n",
    "    \"\"\"\n",
    "    Parse wiki table to get column values and their's hyperlinks\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: str\n",
    "        http-like string url to wiki page\n",
    "    \n",
    "    table_num: int\n",
    "        number of table on page\n",
    "        \n",
    "    col_num: int\n",
    "        number of column to get values from\n",
    "        \n",
    "    return_all: bool, default False\n",
    "        flag, specifying particular page with different xpath\n",
    "    \n",
    "    spec_xpath: str, default None\n",
    "        specify xpath to the table values that needs to be gathered\n",
    "    \n",
    "    \"\"\"\n",
    "    html = rq.get(url)\n",
    "    doc = lxml.html.fromstring(html.content)\n",
    "    \n",
    "    if return_all:\n",
    "        xpth = '//*[@id=\"mw-content-text\"]/div[1]/ul[*]/li[*]/a'\n",
    "        names = doc.xpath(f'{xpth}/text()')\n",
    "        urls = doc.xpath(f'{xpth}/@href')\n",
    "        return names, ['https://en.wikipedia.org'+url for url in urls]\n",
    "    \n",
    "    if spec_xpath:\n",
    "        names = doc.xpath(f'{spec_xpath}/text()') \n",
    "        urls = doc.xpath(f'{spec_xpath}/@href')\n",
    "        return names, ['https://en.wikipedia.org'+url for url in urls]\n",
    "    \n",
    "    xpth = f'//*[@id=\"mw-content-text\"]/div[1]/table[{table_num}]/tbody/tr[*]/td[{col_num}]/a'\n",
    "    names = doc.xpath(f'{xpth}/text()')\n",
    "    urls = doc.xpath(f'{xpth}/@href')\n",
    "    \n",
    "    return names, ['https://en.wikipedia.org'+url for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_companies(source='pulkovo', mode='urls', headers=headers) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse list of companies cooperating with \n",
    "    Pulkovo from the airport website.\n",
    "    Or gather companies from different \n",
    "    wiki page through specifying source parameter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source: {'pulkovo', 'us', 'europe', 'cac', 'africa'\n",
    "            , 'asia', 'northamerica', 'oceania', 'southamerica',\n",
    "            'all', 'test', 'national, 'pulkovo_preloaded'}, default 'pulkovo'\n",
    "        Available sources to get info about airline companies. \n",
    "        There are number of regions available.\n",
    "        NOTE! 'cac' stands for \"Central america and the Caribbean\".\n",
    "        NOTE! You can specify only 'names' if the source=='pulkovo'.\n",
    "        NOTE! You can use source=='pulkovo_preloaded', only if you have appropriate dataset.\n",
    "        \n",
    "    mode: {'names', 'urls'}, default 'urls'\n",
    "        What data to return\n",
    "        NOTE! You can specify only 'names' if the source=='pulkovo'\n",
    "    \n",
    "    headers: dict\n",
    "        Headers to use for the url request \n",
    "        (necessary for avoiding spam-block)\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    if source == 'pulkovo':\n",
    "        \n",
    "        url='https://pulkovoairport.ru/passengers/destinations/airlines/#'\n",
    "        \n",
    "        response = rq.get(url, headers=headers)\n",
    "        to_parse = response.text\n",
    "        soup = BeautifulSoup(to_parse, 'html.parser')\n",
    "        companies = soup.find('ul', {'class': 'tm-air uk-accordion'}).find_all('a', {'class': 'uk-accordion-title'})\n",
    "        names = []\n",
    "        for company in companies:\n",
    "            names.append(company.text)\n",
    "    \n",
    "    elif source == 'pulkovo_preloaded':\n",
    "        plk = pd.read_excel('pulkovo_preloaded.xlsx')\n",
    "        names, urls = plk.names.tolist(), plk.urls.tolist()\n",
    "        \n",
    "    elif source == 'us':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_airlines_of_the_United_States', table_num=3, col_num=1) #us\n",
    "        \n",
    "    elif source == 'europe':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Europe', table_num=1, col_num=2) #eu\n",
    "        \n",
    "    elif source == 'cac':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Central_America_and_the_Caribbean', table_num=2, col_num=2) #cac\n",
    "    \n",
    "    elif source == 'africa':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Africa', table_num=2, col_num=2) #africa\n",
    "    \n",
    "    elif source == 'asia':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Asia','',2, spec_xpath='//*[@id=\"mw-content-text\"]/div[1]/div[1]/table/tbody/tr[*]/td[2]/a')\n",
    "    \n",
    "    elif source == 'northamerica':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_North_America', table_num=2, col_num=1) #na\n",
    "    \n",
    "    elif source == 'oceania':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_Oceania', table_num=1, col_num=1) #oceania\n",
    "        \n",
    "    elif source == 'southamerica':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_largest_airlines_in_South_America', table_num=1, col_num=2) #asia\n",
    "    \n",
    "    elif source == 'all':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_passenger_airlines',0,0,True)\n",
    "        \n",
    "    elif source == 'test':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/List_of_airline_codes', 1, 3)\n",
    "        \n",
    "    elif source == 'national':\n",
    "        names, urls = wiki_urls('https://en.wikipedia.org/wiki/Flag_carrier', 1, 2)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        return names\n",
    "        \n",
    "    if mode == 'urls':\n",
    "        print(f'Number of URLs to iterate through: {len(urls)}')\n",
    "        return urls\n",
    "    \n",
    "    if mode == 'names':\n",
    "        print(f'Number of NAMES to search and iterate through: {len(names)}')\n",
    "        return names\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78efa0b",
   "metadata": {},
   "source": [
    "## Algorithm for searching company's article URL by its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(company: str, mode='ru', append='+авиакомпания') -> Tuple[str]:\n",
    "    \"\"\"\n",
    "    Construct the search query to look through wikipedia.org and \n",
    "    get the title and referer url of the first most apropriate result\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company: str\n",
    "        Name of company to use for constructing a search query\n",
    "    \n",
    "    mode: str\n",
    "        Type of wikipedia library language to use for search\n",
    "\n",
    "    append: str\n",
    "        Keywords to use for constructing a search \n",
    "        query for better accuracy of received result.\n",
    "        NOTE! use + in start of a string and as a separator \n",
    "        between words in append\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    company = company.replace('«','%22').replace('»','%22').replace('Group', '').replace('Holdings', '')\n",
    "    search = '+'.join(company.split())\n",
    "    query = f'https://{mode}.wikipedia.org/w/index.php?search={search}{append}&title=Special:Search&profile=advanced&fulltext=1&ns0=1'\n",
    "    response = rq.get(query, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    href = soup.find('div', {\"class\":\"mw-search-result-heading\"}).find('a').get('href')\n",
    "    title = soup.find('div', {\"class\":\"mw-search-result-heading\"}).find('a').get('title')\n",
    "    \n",
    "    return title, href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(company: str) -> str:\n",
    "    \"\"\"\n",
    "    Compare the result and try to get the \n",
    "    most valuable result with desired company \n",
    "    url to parse further\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company: str\n",
    "        Name of company to look through\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        ru_flag = False\n",
    "        pat = re.compile(company, re.IGNORECASE)\n",
    "        ru = re.compile(\"[а-яА-Я]\")\n",
    "        en = re.compile(\"[a-zA-Z]\")\n",
    "        ru_filter = filter(ru.match, company)\n",
    "        en_filter = filter(en.match, company)\n",
    "        if list(ru_filter):\n",
    "\n",
    "            title, href = process_query(company)\n",
    "            ru_flag = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            title, href = process_query(company, mode='en', append='+airline')\n",
    "\n",
    "        if not re.match(pat, title):\n",
    "\n",
    "            if ru_flag:\n",
    "\n",
    "                title, href = process_query(company, mode='ru', append='')\n",
    "\n",
    "            else:\n",
    "\n",
    "                title, href = process_query(company, mode='en', append='')\n",
    "\n",
    "        if ru_flag:      \n",
    "            link_ = f'https://ru.wikipedia.org{href}'\n",
    "        else:\n",
    "            link_ = f'https://en.wikipedia.org{href}'\n",
    "\n",
    "        response_url = rq.get(link_, headers=headers)\n",
    "        soup_url = BeautifulSoup(response_url.text, 'html.parser')\n",
    "        link = soup_url.find('a', {'class':'interlanguage-link-target', 'lang':'en'})\n",
    "        link = link.get('href') if link is not None else link_\n",
    "\n",
    "        return link\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e0f913",
   "metadata": {},
   "source": [
    "# Processing and transformation of parsed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0345073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_name(response) -> str:\n",
    "    \"\"\"\n",
    "    Get the full name of the company from the title of its wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.Response\n",
    "        response corresponding to particular wikipage\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text[:15000], 'html.parser') \\\n",
    "        .find('h1' ,{\"class\" :\"firstHeading mw-first-heading\"}).text\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_multiindex(df):\n",
    "    \"\"\"\n",
    "    Transform multiindex of dataframe to get pretty looking column names\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Dataframe whose columns needs to be processed\n",
    "    \n",
    "    \"\"\"\n",
    "    def unique_list(l):\n",
    "        ulist = []\n",
    "        [ulist.append(x.strip('01234567890 []')) for x in l if x not in ulist]\n",
    "        return ulist\n",
    "    new_cols = []\n",
    "    if df.columns.nlevels > 1:\n",
    "        df.columns = df.columns.map('|'.join)\n",
    "    else:\n",
    "        df.columns = df.columns.map(str).map(''.join)\n",
    "    for col in df.columns:\n",
    "        splt = col.split('|')\n",
    "        new_cols.append(' '.join(unique_list(splt)))\n",
    "    \n",
    "    return new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83664d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns(df: Iterable) -> tuple:\n",
    "    \"\"\"\n",
    "    Process columns of given DataFrame to get only necessary and apropriate columns. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Dataframe, whose columns should be processed\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    dct = {'Aircraft': [], 'In service': [], 'Orders': [], 'Total_Passengers': [], 'Notes': [], }\n",
    "    match = ('Aircraft|Aircraft type|Type|Plane Name|Тип самолёта',\n",
    "             'In service|Inservice|In operation|No. of aircraft|No. ofaircraft|In fleet|^Fleet$|InFleet|^Total$|Number in Operation|^Number$|^Count$|In-service|Fleet (TC/PLR)|TC list|Active|Эксплуатируется',\n",
    "             'Order|Orders|Заказано',\n",
    "             'Passengers|PassengerTotal|^Passenger|Passengers(Economy)|Passenger capacity|Seating Class|Seats',\n",
    "             'Notes|Additional|Примечания')\n",
    "    \n",
    "    df.columns = handle_multiindex(df)\n",
    "    \n",
    "    for pat, pattern in enumerate(match):\n",
    "\n",
    "        for num, col in enumerate(df.columns):\n",
    "\n",
    "            if re.search(re.compile(pattern, re.IGNORECASE), str(col)):\n",
    "                dct[list(dct.keys())[pat]] = dct.get(list(dct.keys())[pat], []) + [(num, col)]\n",
    "\n",
    "    to_keep = []\n",
    "    for key, values in dct.items():\n",
    "\n",
    "        if len(values)>0:\n",
    "\n",
    "            if key == 'Aircraft':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "            if key == 'In service':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "            if key == 'Orders':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "            if key == 'Total_Passengers':\n",
    "                ref = False\n",
    "                for val in values:\n",
    "                    if 'Ref.' in val[1] or 'Refs' in val[1]:\n",
    "                        ref = True\n",
    "                if ref:\n",
    "                    to_keep.append(values[-2][0])\n",
    "                else:\n",
    "                    to_keep.append(values[-1][0])    \n",
    "\n",
    "            if key == 'Notes':\n",
    "                to_keep.append(values[0][0])\n",
    "\n",
    "\n",
    "\n",
    "    all_idx = [i for i in range(len(df.columns))]\n",
    "    to_del = list(set(all_idx)-set(to_keep))\n",
    "    cols = df.columns[to_keep]\n",
    "    return cols, to_del, to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desigs(response, cut=20000):\n",
    "    \"\"\"\n",
    "    Try to get designator values (IATA, ICAO, Callsign) from response of parcticular wikipage\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.Response\n",
    "        Response from parcticular wikipage\n",
    "    \n",
    "    cut: int\n",
    "        How much text to leave for getting the designators (influences the perfomance)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        soup_nick = BeautifulSoup(response.text[:cut], 'html.parser').find('td', {'class': 'infobox-full-data'}).find_all('td', {'class':'nickname'})\n",
    "        iata = re.sub(r\"\\[.*?\\]\", '', soup_nick[0].text.strip())\n",
    "        icao = re.sub(r\"\\[.*?\\]\", '', soup_nick[1].text.strip())\n",
    "        callsign = re.sub(r\"\\[.*?\\]\", '', soup_nick[2].text.strip())\n",
    "        return iata, icao, callsign\n",
    "    except:\n",
    "        return np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c43230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_table(df: pd.DataFrame, company: str, response) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform given DataFrame table to keep only \n",
    "    necessary info, get the appropriate shape and \n",
    "    unify to a common table form  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame to handle with\n",
    "    \n",
    "    company: str\n",
    "        Company name to use in table\n",
    "    \n",
    "    response: requests.Response\n",
    "        Response from particular wikipage\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    cols, to_del, to_keep = transform_columns(df)\n",
    "\n",
    "    df = df.iloc[:, to_keep].copy()\n",
    "\n",
    "    df.columns = df.columns.map(''.join)\n",
    "\n",
    "    df.columns = cols\n",
    "    add_columns(df) \n",
    "    rename_df_cols(df)\n",
    "\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace(re.compile('\\[.*?\\]'),'').replace(re.compile('[0-9a]*/'), '')\n",
    "    \n",
    "    iata, icao, callsign = get_desigs(response)\n",
    "    \n",
    "    df['Company'] = company\n",
    "    df['Wiki URL'] = response.url\n",
    "    df['Airline IATA'] = iata\n",
    "    df['Airline ICAO'] = icao\n",
    "    df['Callsign'] = callsign\n",
    "    \n",
    "    try:\n",
    "        df = df[(~df.Aircraft.str.lower().str.contains('total'))&(~df.Aircraft.str.lower().str.contains('всего'))&(~df.Aircraft.str.lower().str.contains('total:'))]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_columns(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Add missing columns to the given DataFrame \n",
    "    to get its shape to complete and apropriate form \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame to handle with\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    add = []\n",
    "    match = ('Aircraft|Aircraft type|Type|Plane Name|Тип самолёта',\n",
    "             'In service|Inservice|No. of aircraft|In fleet|^Fleet$|^Total$|InFleet|No. ofaircraft|In operation|In-service|Number in Operation|TC list|^Count$|Fleet (TC/PLR)|^Number$|Active|Эксплуатируется',\n",
    "             'Order|Orders|Заказано',\n",
    "             'Passengers|Passengers(Economy)|^Passenger|PassengerTotal|Passenger capacity|Seating Class|Seats',\n",
    "             'Notes|Additional|Примечания')\n",
    "    \n",
    "    required = ['AirCraft', 'In service', 'Orders', 'Total_Passengers', 'Notes']\n",
    "    for pat, pattern in enumerate(match):\n",
    "        status = False\n",
    "        \n",
    "        for num, col in enumerate(df.columns):\n",
    "            if re.search(re.compile(pattern, re.IGNORECASE), str(col)):\n",
    "                status = True\n",
    "        if not status:\n",
    "            add.append(pat)\n",
    "            \n",
    "    for item in sorted(list(set(add))):\n",
    "        length = len(df.columns)\n",
    "        if item < length:\n",
    "            df.insert(item, required[item], np.nan)\n",
    "        else:\n",
    "            df[required[item]] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_df_cols(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Rename the columns of given DataFrame.\n",
    "    DataFrame should be of specific form\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame whose columns to rename\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['Aircraft','In service', 'Orders', 'Total_Passengers','Notes']\n",
    "    df.columns = cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b7f2b",
   "metadata": {},
   "source": [
    "# Parse engine implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_txt(response, headers: dict, id_value=re.compile('Флот|Fleet'), one_more_url=False) -> str:\n",
    "    \"\"\"\n",
    "    Extract text containing information about aviacompany\n",
    "    fleet from the \"Fleet\" subtitle of wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.Response\n",
    "        Response from particular wikipage\n",
    "\n",
    "    headers: dict\n",
    "        headers to post on the wiki server\n",
    "\n",
    "    id_value: re.compile(..) object, default re.compile('Флот|Fleet')\n",
    "        id_value of \"Fleet\" subtitle to find desired text\n",
    "        \n",
    "    one_more_url: bool, default False\n",
    "        Specifies if table might be on a different page\n",
    "\n",
    "    \"\"\"\n",
    "    if one_more_url:\n",
    "        response = rq.get(response, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    h2_tags = soup.find_all('h2')\n",
    "\n",
    "    for index, h2_tag in enumerate(h2_tags):\n",
    "        span_tag = h2_tag.find('span', {'id': id_value})\n",
    "        if span_tag:\n",
    "            break\n",
    "\n",
    "    start_index = index\n",
    "    finish_index = start_index + 1\n",
    "    start_pattern = h2_tags[start_index].find('span')\n",
    "    finish_pattern = h2_tags[finish_index].find('span')\n",
    "    start = re.search(str(start_pattern), response.text).span()[0]\n",
    "    finish = re.search(str(finish_pattern), response.text).span()[1]\n",
    "    text = response.text[start:finish]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9d748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table(text: str, url, header=None, match='.+', one_more_url=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract pd.DataFrame table from the given html-like string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        HTML string to get table from,\n",
    "        consisting of <table>, <th>, <td> tags\n",
    "\n",
    "    response (url): requests.Response\n",
    "        Response from particular wikipage\n",
    "\n",
    "    header: int, default None\n",
    "        row index to specify header row and\n",
    "        start table\n",
    "\n",
    "    match: str, or re.compile object, default '.+'\n",
    "        pattern to specify table search\n",
    "        \n",
    "    one_more_url: bool, default False\n",
    "        Specifies if table might be on a different page\n",
    "\n",
    "    \"\"\"\n",
    "    if not one_more_url:\n",
    "        url = url.url\n",
    "\n",
    "    if 'ru.wikipedia.org' in url:\n",
    "        df = pd.read_html(text, header=0, match=match)\n",
    "    else:\n",
    "        df = pd.read_html(text, header=header, match=match)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_table(response, company: str, headers: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Extract no-table elements with information\n",
    "    about fleet and convert it to specific form DataFrame,\n",
    "    containing complete information about some\n",
    "    company fleet from its Wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.Response\n",
    "        Response from specific wikipage\n",
    "\n",
    "    company: str\n",
    "        name of the company to use for\n",
    "        forming DataFrame\n",
    "\n",
    "    headers: dict\n",
    "        headers to post on the wiki server\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        info = []\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        text = extract_txt(response, headers)\n",
    "\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "        if soup.find('li') and soup.find('ul'):\n",
    "\n",
    "            soup_ul = BeautifulSoup(text, 'html.parser').find_all('ul')\n",
    "\n",
    "            for ul in soup_ul:\n",
    "                for li in ul.find_all('li'):\n",
    "                    info.append(li.text)\n",
    "        \n",
    "        # ------------------------------------\n",
    "            crafts = []\n",
    "            pat = r'^([0-9])*| – ([0-9]*)'\n",
    "            pat2 = r\"\\(.*?\\)\"\n",
    "            notes = ['' for craft in info]\n",
    "            for ind in range(len(info)):\n",
    "                notes[ind] = re.search(pat2, info[ind]).group(0) if re.search(pat2, info[ind]) else ''\n",
    "                if not notes[ind]:\n",
    "                    notes[ind] = ', '.join(info[ind].rsplit(',')[1:]) if len(info[ind].rsplit(',', 1)) > 1 else ''\n",
    "                if not notes[ind]:\n",
    "                    notes[ind] = info[ind].rsplit(';', 1)[1] if len(info[ind].rsplit(',', 1)) > 1 else ''\n",
    "                if not notes[ind]:\n",
    "                    notes[ind] = info[ind].rsplit(':', 1)[1] if len(info[ind].rsplit(',', 1)) > 1 else ''\n",
    "                if notes[ind]:\n",
    "                    add_pat = notes[ind]\n",
    "                else:\n",
    "                    add_pat = 'nopattern'\n",
    "                pat3 = re.compile(f'{pat2}|{add_pat}')\n",
    "                info[ind] = re.sub(pat3, '', info[ind]).strip(' , ;')\n",
    "                if ';' in info[ind]:\n",
    "                    info[ind] = info[ind].rsplit(';',1)[0].strip()\n",
    "            for craft in info:\n",
    "                mod = re.split(pat, craft)\n",
    "                mod1 = [craft.lstrip(' x×–-\\n').rstrip('-\\n') for craft in mod if craft]\n",
    "                mod2 = sorted(mod1)\n",
    "                crafts.append(mod2)\n",
    "            [craft.insert(0, \"1\") for craft in crafts if len(craft)==1]\n",
    "            table = pd.concat([pd.DataFrame(crafts), pd.Series(notes)], axis=1)\n",
    "            table.columns = ['In service', 'Aircraft', 'Notes']\n",
    "            table = table[['Aircraft', 'In service', 'Notes']]\n",
    "            table = transform_table(table, company, response)\n",
    "            return table, 'table'\n",
    "        # -------------------------------------\n",
    "\n",
    "        else:\n",
    "\n",
    "            info.append(soup.text.replace('\\n', ' '))\n",
    "\n",
    "        df['Parsed info'] = info\n",
    "        df['Company'] = company\n",
    "        df['Index'] = range(1, len(df) + 1)\n",
    "        df.set_index(['Company', 'Index'], inplace=True)\n",
    "\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].replace(re.compile('\\[.*?\\]'), '').replace(re.compile('[0-9]*/'), '')\n",
    "\n",
    "        got = True\n",
    "        \n",
    "        return df, 'text'\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        return None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4908e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(response, headers: dict, extra_page=False) -> tuple:\n",
    "    \"\"\"\n",
    "    Get the table from company's wiki page response\n",
    "    Function firstly extracts text of subtitle 'Fleet'\n",
    "    through extract_txt(..) then extracts table from\n",
    "    the received text with extract_table(...)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.Response\n",
    "        Response from specific wikipage\n",
    "\n",
    "    headers: dict\n",
    "        headers to post on the wiki server\n",
    "\n",
    "    extra_page: bool, default False\n",
    "        flag, used to rerun function with searching\n",
    "        additional fleet url and specifying different\n",
    "        id_value parameter for extract_txt(..)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "\n",
    "        if not extra_page:\n",
    "            text = extract_txt(response, headers, id_value=re.compile('Флот|Fleet|Operators_and_fleet|Current_fleet'))\n",
    "            table = extract_table(text, response)\n",
    "\n",
    "        else:\n",
    "            text = extract_txt(response, headers)\n",
    "            extra_soup = BeautifulSoup(text, 'html.parser')\n",
    "            href = extra_soup.find('div', {\"class\": \"hatnote navigation-not-searchable\"}).find('a').get('href')\n",
    "            new_url = f'https://en.wikipedia.org{href}'\n",
    "            text = extract_txt(new_url, headers, id_value=re.compile('Current'), one_more_url=True)\n",
    "            table = extract_table(text, new_url, one_more_url=True)\n",
    "        \n",
    "        got = True\n",
    "\n",
    "    except:\n",
    "        table = None\n",
    "        got = False\n",
    "\n",
    "    return table, got"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6492638",
   "metadata": {},
   "source": [
    "## Logging the parse process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ea5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(company: str, response: str, urls: list, num, status) -> int:\n",
    "    \"\"\"\n",
    "    Log the tray of parser's work state\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    company: str\n",
    "        Name of the company to use in log\n",
    "        \n",
    "    response: requests.Response\n",
    "        Response from specific wikipage\n",
    "    \n",
    "    responses: list\n",
    "        List of all response from wikipage of companies that need to be parsed\n",
    "        \n",
    "    num: int\n",
    "        Number of response in list responses\n",
    "    \n",
    "    status: str\n",
    "        Some additional comments\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(company) \n",
    "    print(f'URL: {response.url}')\n",
    "    print(f'{num+1}/{len(urls)} URLs parsed')\n",
    "    print(f'Status: {status}')\n",
    "    print(*['-' for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcd367",
   "metadata": {},
   "source": [
    "## Parse exceptions handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exception_handler(df: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Work with the occuring exceptions of \n",
    "    obtained tables from some company wiki page\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: list\n",
    "        List, containing possible desired DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    na = pd.DataFrame()\n",
    "    df1 = df[0] if len(df) > 0 else na\n",
    "    try:    \n",
    "        if 0 in df1.columns or 1 in df1.columns:\n",
    "            for i in range(len(df)):\n",
    "                df1 = df[i] if len(df) > 0 else na\n",
    "                if 0 not in df1.columns or 1 not in df1.columns:\n",
    "                    break\n",
    "            else:\n",
    "                df1 = df[0]\n",
    "                df1 = df1.rename(columns=df1.iloc[0]).drop(df1.index[0])\n",
    "    except:\n",
    "        \n",
    "        df1 = na\n",
    "        \n",
    "    if df1.empty:\n",
    "        \n",
    "        df1 = na\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68cb12",
   "metadata": {},
   "source": [
    "## Separate function for parsing through one URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37db1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url: str, urls: list, headers=headers) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Function, copying the parser logic, but with the single response\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    response (url): requests.Response\n",
    "        Response from specific wikipage\n",
    "    \n",
    "    headers: dict\n",
    "        Headers to post on wiki server\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    fleet = []\n",
    "    corrupted = []\n",
    "    exceptions = []\n",
    "\n",
    "    df = []\n",
    "    company = get_company_name(url)\n",
    "    \n",
    "    df, got = get_table(url, headers)\n",
    "\n",
    "    if not got:\n",
    "        \n",
    "        df, got = get_table(url, headers, extra_page=True)\n",
    "            \n",
    "    if not got:\n",
    "        \n",
    "        df1, got = get_no_table(url, company, headers)\n",
    "        \n",
    "        if got == 'table':\n",
    "            fleet.append(df1)   \n",
    "            logging(company, url, urls, num, status='Non-table transformed to table and parsed')\n",
    "            return df1\n",
    "        \n",
    "        if got == 'text':\n",
    "            logging(company, url, urls, num, status='Non-table text exception')\n",
    "            exceptions.append(df1)\n",
    "            return df1\n",
    "   \n",
    "    if not got:\n",
    "        logging(company, url, urls, num, status='Corrupted')\n",
    "        print(\"Not able to parse data\")\n",
    "        corrupted.append(url.url)\n",
    "        return None\n",
    "    \n",
    "    df1 = exception_handler(df)\n",
    "    \n",
    "    df1 = transform_table(df1, company, url)\n",
    "        \n",
    "    fleet.append(df1)   \n",
    "    logging(company, url, urls, num, status='Table parsed')\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba495af",
   "metadata": {},
   "source": [
    "## Separate function for parsing through one company name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22da4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fleet(company: str, urls: list, headers: dict) -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Function, copying the parser and searcher logic, but with the single response\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    company: str\n",
    "        Company name to search its page on wiki and parse through\n",
    "    \n",
    "    headers: dict\n",
    "        Headers to post on wiki server\n",
    "    \n",
    "    \"\"\"\n",
    "    url = get_url(company)\n",
    "    dataframe = parse(url, urls, headers)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb7af2",
   "metadata": {},
   "source": [
    "## Setting parse objects parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46649e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_urls(parse_obj=[], mode='urls'):\n",
    "    \"\"\"\n",
    "    Function, getting urls to use in parser\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parse_obj: list\n",
    "        List of objects to proceed\n",
    "        \n",
    "    mode: {'urls', 'names'}, default 'urls'\n",
    "        Mode pointing how to deal with parse_obj. If 'urls' specified, function serves as adapter \n",
    "        for get_list_companies(), if 'names' specified function searches names from parse_obj to\n",
    "        get their wiki urls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if mode == 'urls':\n",
    "        \n",
    "        return parse_obj\n",
    "    \n",
    "    if mode == 'names':\n",
    "        urls = [get_url(company) for company in tqdm.tqdm(parse_obj)]\n",
    "        return urls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd859de7",
   "metadata": {},
   "source": [
    "## Speeding up parsing with asynchronous requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c77da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_responses(urls, chunk_len=300, timeout=3, chunk=False):\n",
    "    \"\"\"\n",
    "    Get responses from given urls. Uses asynchronous requests\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    urls: list\n",
    "        List of urls to get responses from\n",
    "        \n",
    "    chunk_len: int, default 800\n",
    "        Size of chunk to proceed if number of URLs is too high\n",
    "        \n",
    "    timeout: int, default 4\n",
    "        How much to wait between each chunk load\n",
    "        \n",
    "    chunk: bool, default False\n",
    "        Manually define if URLs should be splitted by chunks\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def exception_handlerr(request, exception):\n",
    "        print(\"Request failed\", request.url)\n",
    "        \n",
    "    def chunks(xs, chunk_len=chunk_len):\n",
    "        chunk_len = max(1, chunk_len)\n",
    "        return (xs[i:i+chunk_len] for i in range(0, len(xs), chunk_len))\n",
    "    \n",
    "    if len(urls) > 1000:\n",
    "        print('Number of URLs is to high to proceed further.\\nSplitting URLs into chunks and loading them one by one...\\n\\n')\n",
    "        chunk = True\n",
    "        \n",
    "    responses = []\n",
    "    if chunk:\n",
    "        for num, chunk in enumerate(chunks(urls, chunk_len=chunk_len)):\n",
    "            rs = (grequests.get(u) for u in chunk)\n",
    "            chunk_responses = grequests.map(rs, size=16, exception_handler=exception_handlerr)\n",
    "            responses += chunk_responses\n",
    "            print(f'Chunk #{num} of length {chunk_len} loaded, awaiting forced timeout of {timeout} seconds')\n",
    "            time.sleep(timeout)\n",
    "    else:\n",
    "        rs = (grequests.get(u) for u in urls)\n",
    "        responses = grequests.map(rs, size=16, exception_handler=exception_handlerr)\n",
    "        \n",
    "    corrupted = []\n",
    "    responses = list(filter(lambda x: x, [response if response else corrupted.append(response) for response in responses])) \n",
    "    print(f'\\nNumber of responded URLs that will be parsed: {len(responses)}\\n')\n",
    "    return responses, corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed354ac3",
   "metadata": {},
   "source": [
    "## Concatenation of gathered tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tables(fleet: list, exceptions: list) -> Tuple[list,list]:\n",
    "    \"\"\"\n",
    "    Concat received parsed and transformed \n",
    "    tables to get the full overview of aviacompanies' \n",
    "    fleet information\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fleet: list\n",
    "        list of special-form DataFrames with \n",
    "        data about aviacompanies' fleet\n",
    "        \n",
    "    exceptions: list\n",
    "        list of special-form DataFrames with \n",
    "        data about aviacompanies' fleet,\n",
    "        that contains information from no-table \n",
    "        aviacompanies' wiki pages\n",
    "        \n",
    "    \"\"\"\n",
    "    overall = pd.concat(fleet) if fleet else None\n",
    "    overall_exp = pd.concat(exceptions) if exceptions else None\n",
    "    \n",
    "    return overall, overall_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0877d2",
   "metadata": {},
   "source": [
    "## Loaders and handlers of external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a235a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_icao():\n",
    "    headers = {\n",
    "        'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
    "        'Accept-Language': 'en,ru-RU;q=0.9,ru;q=0.8,en-US;q=0.7',\n",
    "        'Connection': 'keep-alive',\n",
    "        # 'Content-Length': '0',\n",
    "        'Origin': 'https://www.icao.int',\n",
    "        'Referer': 'https://www.icao.int/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-site',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\".Not/A)Brand\";v=\"99\", \"Google Chrome\";v=\"103\", \"Chromium\";v=\"103\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response = rq.post('https://www4.icao.int/doc8643/External/AircraftTypes', headers=headers)\n",
    "    data = response.json()\n",
    "    icao_codes = pd.DataFrame(data)\n",
    "    icao_codes.ManufacturerCode = icao_codes.ManufacturerCode.str.replace(' \\([1-9]*\\)', '', regex=True)\n",
    "    icao_codes['FullName'] = icao_codes.ManufacturerCode.str.lower().str.capitalize() +\" \" + icao_codes.ModelFullName\n",
    "    # a.columns[[9,5,0,4,1,6,7,8,2,3]]\n",
    "    icao_codes = icao_codes[['FullName', 'ManufacturerCode', 'ModelFullName', 'Designator',\n",
    "           'Description', 'AircraftDescription', 'EngineCount', 'EngineType',\n",
    "           'WTC', 'WTG']]\n",
    "    icao_codes = icao_codes.rename(columns={'Designator':'ICAO'})\n",
    "    icao_codes.FullName = icao_codes.FullName.str.lower().str.capitalize()\n",
    "    print('Dataset \"icao\" from https://www4.icao.int/doc8643/External/AircraftTypes has been gathered and processed')\n",
    "    return icao_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapper(preloaded=False):\n",
    "    if preloaded:\n",
    "        return pd.read_excel('mapper_codes.xlsx')\n",
    "    mapper_codes = pd.read_html('https://www.greatcirclemapper.net/en/aircrafts.html')[0]\n",
    "    mapper_codes['FullName'] = mapper_codes.Manufacturer + ' ' + mapper_codes.Name\n",
    "    mapper_codes.Manufacturer = mapper_codes.Manufacturer.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    mapper_codes.Name = mapper_codes.Name.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    mapper_codes = mapper_codes[['Manufacturer', 'Name', 'IATA', 'Type']]\n",
    "    manufacturers = mapper_codes.Manufacturer.tolist()\n",
    "    names = mapper_codes.Name.tolist()\n",
    "    mapper_codes = mapper_codes.fillna('-')\n",
    "    resp = rq.get('https://www.greatcirclemapper.net/en/aircrafts.html')\n",
    "    sp = BeautifulSoup(resp.text, 'html.parser')\n",
    "    crafts_pages = []\n",
    "    for craft in sp.find('table').find_all('strong'):\n",
    "        if craft.find('a'):\n",
    "            craft_page = craft.find('a').get('href')\n",
    "            craft_page = f'https://www.greatcirclemapper.net/{craft_page}'\n",
    "            crafts_pages.append(craft_page)\n",
    "        else:\n",
    "            crafts_pages.append(None)\n",
    "    icaos_aircraft_mapper = []\n",
    "    responses, _ = get_responses(crafts_pages)\n",
    "    for response in tqdm.tqdm(responses):\n",
    "        if response:\n",
    "            soup_obj = BeautifulSoup(response.text, 'html.parser')\n",
    "            icao_mapper = soup_obj.find('dt', string='ICAO').find_next_sibling().text.strip()\n",
    "            icaos_aircraft_mapper.append(icao_mapper)\n",
    "        else:\n",
    "            icaos_aircraft_mapper.append(None)\n",
    "    mapper_codes['ICAO'] = icaos_aircraft_mapper\n",
    "\n",
    "    pat_split = ' / |, |/'\n",
    "    frames = []\n",
    "    for ind, obj in enumerate(manufacturers):\n",
    "        manufacturers[ind] = re.split(pat_split, obj)\n",
    "\n",
    "    mapper_codes['Manufacturer'] = manufacturers\n",
    "\n",
    "    for ind, obj in enumerate(names):\n",
    "        names[ind] = re.split(pat_split, obj)\n",
    "\n",
    "    mapper_codes['Name'] = names\n",
    "    mapper_codes.IATA = mapper_codes.IATA.str.split('      ')\n",
    "    mapper_codes.Type = mapper_codes.Type.str.split('      ')\n",
    "    mapper_codes.ICAO = mapper_codes.ICAO.str.split('      ')\n",
    "    for row in range(len(mapper_codes)):\n",
    "        for element in itertools.product(*mapper_codes.values[row]):\n",
    "            frames.append(pd.DataFrame([element], columns = mapper_codes.columns))\n",
    "\n",
    "    mapper_codes = pd.concat(frames)\n",
    "    mapper_codes.Manufacturer = mapper_codes.Manufacturer.str.replace(' Aircraft', ' ')\n",
    "    mapper_codes['FullName'] = mapper_codes.Manufacturer + ' ' + mapper_codes.Name\n",
    "    mapper_codes = mapper_codes.drop_duplicates()\n",
    "    mapper_codes = mapper_codes[['FullName', 'Manufacturer', 'Name','ICAO', 'IATA', 'Type']]\n",
    "    mapper_codes.FullName = mapper_codes.FullName.str.lower().str.capitalize()\n",
    "    mapper_codes = mapper_codes[~mapper_codes['FullName'].duplicated()]\n",
    "    mapper_codes.to_excel('mapper_codes.xlsx', index=False)\n",
    "    print('Dataset \"mapper\" from https://www.greatcirclemapper.net/en/aircrafts.html has been gathered and processed')\n",
    "    return mapper_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab134921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki1():\n",
    "    wiki_codes1 = pd.read_html('https://en.wikipedia.org/wiki/List_of_aircraft_type_designators')[0]\n",
    "    wiki_codes1.columns = ['ICAO', 'IATA', 'FullName']\n",
    "    wiki_codes1 = wiki_codes1[['FullName', 'ICAO', 'IATA']]\n",
    "    wiki_codes1.FullName = wiki_codes1.FullName.str.lower().str.capitalize()\n",
    "    wiki_codes1 = wiki_codes1[~wiki_codes1['FullName'].duplicated()]\n",
    "    print('Dataset \"wiki1\" from https://en.wikipedia.org/wiki/List_of_aircraft_type_designators has been gathered and processed')\n",
    "    return wiki_codes1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki2():\n",
    "    resp2 = rq.get('https://en.wikipedia.org/wiki/List_of_civil_aircraft')\n",
    "    sp2 = BeautifulSoup(resp2.text, 'html.parser')\n",
    "    wiki_codes2 = []\n",
    "    for i in sp2.find_all('a', {'href': re.compile('/wiki/')})[2:1856]:\n",
    "        wiki_codes2.append(i.text)\n",
    "    wiki_codes2 = pd.DataFrame(wiki_codes2, columns=['FullName'])\n",
    "    wiki_codes2.FullName = wiki_codes2.FullName.str.lower().str.capitalize()\n",
    "    wiki_codes2 = wiki_codes2[~wiki_codes2['FullName'].duplicated()]\n",
    "    print('Dataset \"wiki2\" from https://en.wikipedia.org/wiki/List_of_civil_aircraft has been gathered and processed')\n",
    "    return wiki_codes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki3():\n",
    "    resp3 = rq.get('https://en.wikipedia.org/wiki/List_of_aircraft_by_date_and_usage_category')\n",
    "    sp3 = BeautifulSoup(resp3.text, 'html.parser')\n",
    "    wiki_codes3 = []\n",
    "    for i in sp3.find('table').find_next('table').find_next('table').find_next('table').find_next('table').find_all('a', {'href': re.compile('/wiki/')}):\n",
    "        wiki_codes3.append(i.text)\n",
    "    wiki_codes3 = pd.DataFrame(wiki_codes3, columns=['FullName'])\n",
    "    wiki_codes3.FullName = wiki_codes3.FullName.str.lower().str.capitalize()\n",
    "    wiki_codes3 = wiki_codes3[~wiki_codes3['FullName'].duplicated()]\n",
    "    print('Dataset \"wiki3\" from https://en.wikipedia.org/wiki/List_of_aircraft_by_date_and_usage_category has been gathered and processed')\n",
    "    return wiki_codes3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki4():\n",
    "    df = pd.read_html('https://en.wikipedia.org/wiki/List_of_airline_codes')[0]\n",
    "#     df = df.rename(columns={'IATA':'IATA_airline', \"ICAO\":\"ICAO_airline\", \"Airline\":\"FullName\"})\n",
    "    df = df.rename(columns={\"Airline\":\"FullName\"})\n",
    "    df.FullName = df.FullName.str.lower().str.capitalize()\n",
    "    df = df[~df.duplicated()]\n",
    "    df = df[~df.FullName.duplicated()]\n",
    "    print('Dataset \"code\" from https://en.wikipedia.org/wiki/List_of_airline_codes has been gathered and processed')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc41d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fan(preloaded=False):\n",
    "    if preloaded:\n",
    "            return pd.read_excel('fan_codes.xlsx')\n",
    "    tables = []\n",
    "    urls = []\n",
    "    for page in range(1, 275):\n",
    "        urls.append(f'https://www.aviationfanatic.com/ent_list.php?ent=4&pg={page}')\n",
    "    responses, _  = get_responses(urls)\n",
    "    for response in responses:\n",
    "        tables.append(pd.read_html(response.text)[0][:-2])\n",
    "    fan_codes = pd.concat(tables)\n",
    "    fan_codes.columns = ['#', 'ID', 'Manufacturer', 'FullName',\n",
    "           '# of related pictures', 'ICAO',\n",
    "           'Manufacturer country', 'Category', 'Role', 'Engine type', 'Engines',\n",
    "           'WTC', 'Seats', 'First flight', 'Last manufactured', 'Total # built',\n",
    "           'Info (external)', '# of aircraft in DB', '# of related collections',\n",
    "           '# of related user comments', 'Unnamed: 20']\n",
    "    fan_codes = fan_codes[['ID', 'Manufacturer', 'FullName', 'ICAO',\n",
    "           'Manufacturer country', 'Category', 'Role', 'Engine type', 'Engines','WTC']]\n",
    "    fan_codes.FullName = fan_codes.FullName.str.lower().str.capitalize()\n",
    "    fan_codes = fan_codes[~fan_codes['FullName'].duplicated()]\n",
    "    fan_codes.to_excel('fan_codes.xlsx', index=False)\n",
    "    print('Dataset \"fan\" from https://www.aviationfanatic.com/ent_list.php?ent=4 has been gathered and processed')\n",
    "    return fan_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cc436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mil():\n",
    "    rspn = rq.get(\"https://www.militaryfactory.com/aircraft/indexMAIN.php\").text\n",
    "    sp = BeautifulSoup(rspn, 'html.parser')\n",
    "    mil_codes = []\n",
    "    for i in sp.find_all('span', {\"class\":\"textLarge textBold textDkGray\"}):\n",
    "        mil_codes.append(i.text)\n",
    "    mil_codes = pd.Series(mil_codes).replace(' \\((.*?)\\)', '', regex=True).str.strip().tolist()\n",
    "    mil_codes = pd.DataFrame(mil_codes, columns=['FullName'])\n",
    "    mil_codes.FullName = mil_codes.FullName.str.lower().str.capitalize()\n",
    "    mil_codes = mil_codes[~mil_codes['FullName'].duplicated()]\n",
    "    print('Dataset \"military\" from https://www.militaryfactory.com/aircraft/indexMAIN.php has been gathered and processed')\n",
    "    return mil_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3d22f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_ssim():\n",
    "    ssim_codes = pd.read_html('http://wiki.aviabit.ru/doku.php?id=pub:ssim._apendix_a')[0]\n",
    "    ssim_codes.columns = ['FullName', 'IATA', 'Group IATA', 'Cate', 'ICAO']\n",
    "    ssim_codes.FullName = ssim_codes.FullName.str.lower().str.capitalize()\n",
    "    ssim_codes.FullName = ssim_codes.FullName.str.replace('passenger', '', regex=False)\n",
    "    ssim_codes = ssim_codes[~ssim_codes['FullName'].duplicated()]\n",
    "    print('Dataset \"iata\" from http://wiki.aviabit.ru/doku.php?id=pub:ssim._apendix_a has been gathered and processed')\n",
    "    return ssim_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flug_icao():\n",
    "    flug_codes = pd.concat(pd.read_html('http://www.flugzeuginfo.net/table_accodes_en.php'))\n",
    "    flug_codes = flug_codes.rename(columns={'Type/Model': 'Name', 'Wake':'WTC'})\n",
    "    flug_codes.Manufacturer = flug_codes.Manufacturer.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    flug_codes.Name = flug_codes.Name.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "\n",
    "    manufacturers = flug_codes.Manufacturer.tolist()\n",
    "    names = flug_codes.Name.tolist()\n",
    "    flug_codes = flug_codes.fillna('-')\n",
    "    pat_split = ' / |, |/'\n",
    "    frames = []\n",
    "    for ind, obj in enumerate(manufacturers):\n",
    "        manufacturers[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Manufacturer'] = manufacturers\n",
    "\n",
    "    for ind, obj in enumerate(names):\n",
    "        names[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Name'] = names\n",
    "\n",
    "    flug_codes.ICAO = flug_codes.ICAO.str.split('      ')\n",
    "    flug_codes.WTC = flug_codes.WTC.str.split('      ')\n",
    "    for row in range(len(flug_codes)):\n",
    "        for element in itertools.product(*flug_codes.values[row]):\n",
    "            frames.append(pd.DataFrame([element], columns = flug_codes.columns))\n",
    "\n",
    "    flug_codes = pd.concat(frames)\n",
    "    flug_codes['FullName'] = flug_codes.Manufacturer + ' ' + flug_codes.Name\n",
    "    flug_codes = flug_codes[['FullName', 'Manufacturer', 'Name', 'ICAO', 'WTC']]\n",
    "    flug_codes = flug_codes[~flug_codes['FullName'].duplicated()]\n",
    "    print('Dataset \"flug_icao\" from http://www.flugzeuginfo.net/table_accodes_en.php has been gathered and processed')\n",
    "    return flug_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flug_iata():\n",
    "    flug_codes = pd.read_html('http://www.flugzeuginfo.net/table_accodes_iata_en.php')[0]\n",
    "    flug_codes = flug_codes.rename(columns={'Type/Model': 'Name', 'Wake':'WTC'})\n",
    "    flug_codes.Manufacturer = flug_codes.Manufacturer.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "    flug_codes.Name = flug_codes.Name.str.replace('(', '', regex=False).str.replace(')', '', regex=False)\n",
    "\n",
    "    manufacturers = flug_codes.Manufacturer.tolist()\n",
    "    names = flug_codes.Name.tolist()\n",
    "    flug_codes = flug_codes.fillna('-')\n",
    "    pat_split = ' / |, |/'\n",
    "    frames = []\n",
    "    for ind, obj in enumerate(manufacturers):\n",
    "        manufacturers[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Manufacturer'] = manufacturers\n",
    "\n",
    "    for ind, obj in enumerate(names):\n",
    "        names[ind] = re.split(pat_split, obj)\n",
    "    flug_codes['Name'] = names\n",
    "\n",
    "    flug_codes.IATA = flug_codes.IATA.str.split('      ')\n",
    "    flug_codes.WTC = flug_codes.WTC.str.split('      ')\n",
    "    for row in range(len(flug_codes)):\n",
    "        for element in itertools.product(*flug_codes.values[row]):\n",
    "            frames.append(pd.DataFrame([element], columns = flug_codes.columns))\n",
    "\n",
    "    flug_codes = pd.concat(frames)\n",
    "    flug_codes['FullName'] = flug_codes.Manufacturer + ' ' + flug_codes.Name\n",
    "    flug_codes = flug_codes[['FullName', 'Manufacturer', 'Name', 'IATA', 'WTC']]\n",
    "    flug_codes = flug_codes[~flug_codes['FullName'].duplicated()]\n",
    "    print('Dataset \"flug_iata\" from http://www.flugzeuginfo.net/table_accodes_iata_en.php has been gathered and processed')\n",
    "    return flug_codes.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac032014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_av():\n",
    "    av_codes = pd.read_html('https://www.avcodes.co.uk/acrtypes.asp')[0].rename(columns={'IATACode':\"IATA\", 'ICAOCode':'ICAO', 'Manufacturer and Aircraft Type / Model': 'FullName'})\n",
    "    av_codes = av_codes[~av_codes['FullName'].duplicated()]\n",
    "    print('Dataset \"av\" from https://www.avcodes.co.uk/acrtypes.asp has been gathered and processed')\n",
    "    return av_codes[~av_codes['FullName'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ea20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_av_airlines(preloaded=False):\n",
    "    if preloaded:\n",
    "        return pd.read_excel('av_airlines.xlsx')\n",
    "    id_start = 23676\n",
    "    id_finish = 29983\n",
    "    urls = [f'https://www.avcodes.co.uk/details.asp?ID={ID}' for ID in range(id_start, id_finish+1)]\n",
    "    resps = get_responses(urls)\n",
    "    rows = []\n",
    "    for num, resp in enumerate(resps):\n",
    "        if not resp:\n",
    "            continue\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        elements = []\n",
    "        for element in soup.find_all('td'): \n",
    "            hhh = element.text.strip().split(':') if element.text else ['']\n",
    "            hhh = hhh[1] if len(hhh)>1 else hhh[0]\n",
    "            elements.append(hhh)\n",
    "        if len(elements)==14:\n",
    "            elements.insert(1, '')\n",
    "        rows.append(elements)\n",
    "    \n",
    "    columns = ['name', 'logo', 'FullName', 'IATA', 'ICAO', 'ICAO_callsign',\n",
    "            'IATA_accounting', 'IATA_prefix', 'Country', 'Website', 'flag',\n",
    "            'Founded', 'commences_ops', 'ceased_ops', 'remarks']\n",
    "    av_airlines = pd.DataFrame(rows, columns=columns)\n",
    "    av_airlines.Website = av_airlines.Website.str.replace('Website URL', '')\n",
    "    av_airlines.FullName = av_airlines.FullName.str.lower().str.capitalize()\n",
    "    av_airlines.to_excel('av_airlines.xlsx', index=False)\n",
    "    print('Dataset \"av_airlines\" from https://www.avcodes.co.uk/details.asp has been gathered and processed')\n",
    "    return av_airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66719123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flug_airlines():\n",
    "    df = pd.concat(pd.read_html('http://www.flugzeuginfo.net/table_airlinecodes_airline_en.php'))\n",
    "    df = df.rename(columns={'Airline':'FullName'})\n",
    "    df.FullName = df.FullName.str.lower().str.capitalize()\n",
    "    print('Dataset \"flug_airlines\" from http://www.flugzeuginfo.net/table_airlinecodes_airline_en.php has been gathered and processed')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aircodes():\n",
    "    df = pd.read_html('https://airlinecodes.info/icao')[0]\n",
    "    df = df.rename(columns={\"NAME\":\"FullName\"})\n",
    "    df.FullName = df.FullName.str.lower().str.capitalize()\n",
    "    df = df[~df.duplicated()]\n",
    "    df = df[~df.FullName.duplicated()]\n",
    "    print('Dataset \"aircodes\" from https://airlinecodes.info/icao has been gathered and processed')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba0e53",
   "metadata": {},
   "source": [
    "## Separate function for loading all external databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd600c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_databases():\n",
    "    ssim_codes = load_ssim()\n",
    "    icao_codes = load_icao()\n",
    "    wiki_codes1 = load_wiki1()\n",
    "    wiki_codes2 = load_wiki2()\n",
    "    wiki_codes3 = load_wiki3()\n",
    "    fan_codes = load_fan(preloaded=True)\n",
    "    mapper_codes = load_mapper(preloaded=True)\n",
    "    mil_codes = load_mil()\n",
    "    flug_icao = load_flug_icao()\n",
    "    flug_iata = load_flug_iata()\n",
    "    av_codes = load_av()\n",
    "    all_codes = {\n",
    "             'iata':ssim_codes, 'icao':icao_codes, 'wiki1': wiki_codes1, \n",
    "             'wiki2':wiki_codes2, 'wiki3':wiki_codes3, 'fan': fan_codes, \n",
    "             'mapper': mapper_codes, 'military': mil_codes, 'flug_iata': flug_iata,\n",
    "             'flug_icao':flug_icao, 'av': av_codes\n",
    "            }\n",
    "    return all_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f403f34a",
   "metadata": {},
   "source": [
    "## Modification difflib.get_close_matches() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b192b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_close_matches(word, possibilities, n=1, cutoff=0.6):\n",
    "    \"\"\"\n",
    "    Modification of difflib.get_close_matches()\n",
    "    Now also return scores, not only the matches\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word: str\n",
    "        Word to find close matches\n",
    "    \n",
    "    possibilities: Iterable\n",
    "        Iterable object, containing possible matches\n",
    "    \n",
    "    cutoff: float, default 0.6\n",
    "        Minimal appropriate match score for matches\n",
    "    \"\"\"\n",
    "\n",
    "    if not n >  0:\n",
    "        raise ValueError(\"n must be > 0: %r\" % (n,))\n",
    "    if not 0.0 <= cutoff <= 1.0:\n",
    "        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n",
    "    result = []\n",
    "    s = SequenceMatcher()\n",
    "    s.set_seq2(word)\n",
    "    for x in possibilities:\n",
    "        s.set_seq1(x)\n",
    "        if s.real_quick_ratio() >= cutoff and \\\n",
    "           s.quick_ratio() >= cutoff and \\\n",
    "           s.ratio() >= cutoff:\n",
    "            result.append((s.ratio(), x))\n",
    "\n",
    "    # Move the best scorers to head of list\n",
    "    result = _nlargest(n, result)\n",
    "    # Strip scores for the best n matches\n",
    "    return [(score, x) for score, x in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad7aa6",
   "metadata": {},
   "source": [
    "## Related function to get_close_matches() that returns only one match with its score (cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa198c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match(x, other, cutoff):\n",
    "    matches = get_close_matches(x, other, n=1, cutoff=cutoff)\n",
    "    return matches[0][0], matches[0][1] if matches else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a1278",
   "metadata": {},
   "source": [
    "## Main algorithm of matching fuzzy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc90f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_merge(df1, df2, left_on, right_on, how='left', cutoff=0.65):\n",
    "    \"\"\"\n",
    "    Make a merge on fuzzy matches\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df1: pd.DataFrame\n",
    "        Left DataFrame to fuzzy merge on\n",
    "        \n",
    "    df2: pd.DataFrame\n",
    "        Right DataFrame to fuzzy match with\n",
    "        \n",
    "    left_on: str\n",
    "        Left DataFrame column name whose values to match\n",
    "        \n",
    "    right_on: str\n",
    "        Right DataFrame column name whose values to fuzzy merge\n",
    "        \n",
    "    how: str, default 'left'\n",
    "        how to proceed merge\n",
    "\n",
    "    cutoff: int, default 0.65\n",
    "        Minimal appropriate match score for matches\n",
    "    \"\"\"\n",
    "    df_other= df1.copy()\n",
    "\n",
    "    left = [None for i in range(len(df1[left_on]))]\n",
    "    scores = [None for i in range(len(df1[left_on]))]\n",
    "    df2 = df2.groupby(by=right_on).first().reset_index().copy()\n",
    "\n",
    "    for ind, x in enumerate(df_other[left_on]):\n",
    "        try:\n",
    "            score, match = get_closest_match(x, df2[right_on][df2[right_on].str.contains(x.split()[0], na=False)], cutoff)\n",
    "        except Exception as Ex:\n",
    "            score = None\n",
    "            match = None\n",
    "        if match and not left[ind]:\n",
    "            left[ind] = match\n",
    "            scores[ind] = score\n",
    "     \n",
    "    df1[right_on] = left\n",
    "    df1['cutoffs'] = scores\n",
    "\n",
    "    return df1.merge(df2, on=right_on, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b70f06",
   "metadata": {},
   "source": [
    "## Deriving and processing column that needs to be fuzzy matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f748a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_match_col(df, col_to_derive, change_values_of, how_to_name='Crafts'):\n",
    "    \"\"\"\n",
    "    Derive and process match column to make fuzzy merge on\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        DataFrame from which to get column\n",
    "    \n",
    "    col_to_derive: str\n",
    "        Name of column to use for matching\n",
    "        \n",
    "    change_values_of: list,\n",
    "        List of values that should be changed for better matching\n",
    "        \n",
    "    how_to_name: str\n",
    "        Name of derived column\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    proxy = df.copy()\n",
    "    \n",
    "    proxy[how_to_name] = overall[col_to_derive]\n",
    "    proxy_deriv = proxy[how_to_name].to_frame().reset_index(drop=True)\n",
    "    proxy_deriv[how_to_name] = proxy_deriv[how_to_name].str.lower().str.capitalize()\n",
    "    \n",
    "    if how_to_name == \"Crafts\":\n",
    "        for change_value in change_values_of:\n",
    "            proxy_deriv[how_to_name][proxy_deriv[how_to_name].str.contains(change_value, na=False)] = \\\n",
    "            proxy_deriv[how_to_name][proxy_deriv[how_to_name].str.contains(change_value, na=False)] \\\n",
    "            .str.replace('(-100)', '', regex=True).str.replace('-200', '').str.replace('-231','') \\\n",
    "            .str.replace('Ан', 'Антонов ан').str.replace('Ми', 'Мил ми')\n",
    "    \n",
    "    trans = lambda x: transliterate.translit(x, 'ru', reversed=True) if x and type(x)==str else np.nan\n",
    "    proxy_deriv[how_to_name] = proxy_deriv[how_to_name].apply(trans)\n",
    "    \n",
    "    return proxy_deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a9656",
   "metadata": {},
   "source": [
    "## Match engine algorithm that performs analysis through all loaded databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b51621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(main_frame, analyzers, left_on='Crafts', right_on='FullName', cutoff=0.65):\n",
    "    \"\"\"\n",
    "    Find matches throughout several datasets to find best matching value\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    main_frame: pd.DataFrame\n",
    "        1-d DataFrame with column to match\n",
    "    \n",
    "    analyzers: list\n",
    "        List, containing tuples of dataset and their specifications\n",
    "        \n",
    "    left_on: str\n",
    "        Left DataFrame column name whose values to match\n",
    "        \n",
    "    right_on: str\n",
    "        Right DataFrame column name whose values to fuzzy merge\n",
    "\n",
    "    cutoff: int, default 0.65\n",
    "        Minimal appropriate match score for matches\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for num, analyzer in enumerate(analyzers):\n",
    "        start = datetime.now()\n",
    "        data = analyzer[1].copy()\n",
    "        print(f\"Iteration #{num}\")\n",
    "        print(f'Set: \"{analyzer[0]}\",  lenght: {len(data)}')\n",
    "        data.FullName = data.FullName.str.lower().str.capitalize().reset_index(drop=True)\n",
    "        main_frame = fuzzy_merge(main_frame, data, left_on=left_on, right_on=right_on, how='left', cutoff=cutoff)\n",
    "        main_frame.rename(columns={'FullName': f'FullName_{num}', 'cutoffs': f'cutoffs_{num}'}, inplace=True)\n",
    "        print(f\"Time taken: {datetime.now()-start}\\nFrame lenght: {len(main_frame)}\")\n",
    "        print(*['-' for i in range(20)])\n",
    "    return main_frame\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d887270",
   "metadata": {},
   "source": [
    "## Derivation of best cutoff(score) through all cutoffs for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_best_score(df, score_col_name='best_cutoff'):\n",
    "    if df.dropna(thresh=2).empty:\n",
    "        df[score_col_name] = np.nan\n",
    "        return None\n",
    "    cutoff_cols = [column for column in df.columns if 'cutoffs' in column]\n",
    "    for col in cutoff_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='ignore')\n",
    "    df[score_col_name] = df[cutoff_cols].idxmax(axis=1)\n",
    "    df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af435759",
   "metadata": {},
   "source": [
    "## Derivation of best matches and their original databases indices for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_best(df, best_cutoff_col):\n",
    "    if df.dropna(thresh=2).empty:\n",
    "        df['best_match'] = np.nan\n",
    "        df['best_match_index'] = np.nan\n",
    "        df['codes_base_num'] = np.nan\n",
    "        return None\n",
    "    df['best_match'] = df[best_cutoff_col][df[best_cutoff_col].str.contains('_', na=False)].replace('cutoffs', 'FullName', regex=True)\n",
    "    df['best_match_index'] = (df['best_match'].str[-1].astype(float)*2 + 1).astype(int, errors='ignore')\n",
    "    df['codes_base_num'] = df['best_match'].str[-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba516aed",
   "metadata": {},
   "source": [
    "## Derivation original databases of best matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_db(df, db_ind_col, db_list):\n",
    "    orig_database = []\n",
    "    for row in df.index:\n",
    "        try:\n",
    "            key = db_list[int(df.loc[row, db_ind_col])][0]\n",
    "        except:\n",
    "            key = np.nan\n",
    "            \n",
    "        orig_database.append(key)\n",
    "    \n",
    "    return orig_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7430b",
   "metadata": {},
   "source": [
    "## Check for designator (ICAO, IATA) availability in original databases for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desig_availability(df, db_col, db_available):\n",
    "    desig_truth = []\n",
    "    for row in df.index:\n",
    "        dataset = df.loc[row, db_col]\n",
    "        try:\n",
    "            have_desig = db_available[dataset]\n",
    "        except:\n",
    "            have_desig = False\n",
    "            \n",
    "        desig_truth.append(have_desig)\n",
    "    \n",
    "    return desig_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52269474",
   "metadata": {},
   "source": [
    "## Derivation of designator (ICAO, IATA) values for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9320e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desig_values(df, db_col, desig_truth_col, dbs_dct, desig):\n",
    "    \n",
    "    desig_codes = []\n",
    "    \n",
    "    for row in df.index:\n",
    "    \n",
    "        dataset = df.loc[row, db_col]\n",
    "\n",
    "        if df.loc[row, desig_truth_col] == True:\n",
    "            match_col = df.loc[row, 'best_match']\n",
    "            match_value = df.loc[row, match_col]\n",
    "            desig_value = dbs_dct[dataset].loc[dbs_dct[dataset]['FullName']==match_value, desig]\n",
    "            try:\n",
    "                desig_value = desig_value.item() if len(desig_value) else np.nan\n",
    "            except:\n",
    "                desig_value = desig_value.iloc[0] if len(desig_value) else np.nan\n",
    "        \n",
    "        else:\n",
    "            desig_value = np.nan\n",
    "            \n",
    "        desig_codes.append(desig_value)\n",
    "        \n",
    "    return desig_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7e050",
   "metadata": {},
   "source": [
    "## Obtaining values (best_match/best_cutoff) for all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(df, column):\n",
    "    if df.dropna(thresh=4).empty:\n",
    "        values = np.nan\n",
    "        return values\n",
    "    values = []\n",
    "    for row in df.index:\n",
    "        val_col = df.loc[row, column]\n",
    "        if val_col is not np.nan:\n",
    "            value = df.loc[row, val_col]\n",
    "        else:\n",
    "            value = np.nan\n",
    "        values.append(value)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96694f9c",
   "metadata": {},
   "source": [
    "## Rematching algorithm that tries to guess unmatched observations one more time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rematching(df, dbs_dct, left_on, right_on, skip_list, icao_list, icaos_aircraft, iatas_aircraft):\n",
    "    print('\\nRematching unfilled values...\\n\\n')\n",
    "    rematch_vals = []\n",
    "    for row in df.index:\n",
    "        no_icao = not df.loc[row, 'have_icao']\n",
    "        rematch_vals.append(no_icao)\n",
    "    rematch_rows = df[rematch_vals].copy()\n",
    "    if rematch_rows.dropna(thresh=4).empty:\n",
    "        return pd.DataFrame(columns=[left_on, 'best_cutoff', 'orig_database', 'best_match',\n",
    "                                     'best_match_index', 'codes_base_num', 'have_icao', 'have_iata',\n",
    "                                     'ICAO', 'IATA', 'researched_value', 'cutoff']\n",
    "                           )\n",
    "\n",
    "    new_matches = []\n",
    "    for row in rematch_rows.index:\n",
    "\n",
    "        new_match_col = rematch_rows.loc[row, 'best_match']\n",
    "        if new_match_col is not np.nan:\n",
    "            new_match = rematch_rows.loc[row, new_match_col]\n",
    "        else:\n",
    "            new_match = np.nan\n",
    "\n",
    "        new_matches.append(new_match)\n",
    "        \n",
    "    rematch_rows['rematch_values'] = new_matches\n",
    "    \n",
    "    rematch_old_index = rematch_rows.index\n",
    "    \n",
    "    rematch_crafts = rematch_rows.rematch_values.to_frame().rename(columns={'rematch_values':'Crafts'})\n",
    "    \n",
    "    dbs_rematch = [(key, dbs_dct[key]['FullName'].to_frame()) for key in dbs_dct \\\n",
    "                   if key not in skip_list and key in icao_list]\n",
    "\n",
    "    rematch_df = analyze(rematch_crafts, dbs_rematch, left_on, right_on, cutoff=0.65)\n",
    "    \n",
    "    rematch_df.index = rematch_old_index\n",
    "    \n",
    "    derive_best_score(rematch_df)\n",
    "    \n",
    "    rematch_df = get_info_best(rematch_df, 'best_cutoff')\n",
    "    \n",
    "    rematch_df['orig_database'] = find_db(rematch_df, 'codes_base_num', dbs_rematch)\n",
    "    \n",
    "    rematch_df['have_icao'] = desig_availability(rematch_df, 'orig_database', icaos_aircraft)\n",
    "    rematch_df['have_iata'] = desig_availability(rematch_df, 'orig_database', iatas_aircraft)\n",
    "    \n",
    "    rematch_df['ICAO'] = get_desig_values(rematch_df, 'orig_database', 'have_icao', dbs_dct, 'ICAO')\n",
    "    rematch_df['IATA'] = get_desig_values(rematch_df, 'orig_database', 'have_iata', dbs_dct, 'IATA')\n",
    "    \n",
    "    rematch_df['researched_value'] = get_values(rematch_df, 'best_match')\n",
    "    rematch_df['cutoff'] = get_values(rematch_df, 'best_cutoff')\n",
    "    return rematch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc13b6",
   "metadata": {},
   "source": [
    "## Matching algorithm that contains all processing and values derivation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching(main_frame, dbs_dct, left_on, right_on, skip_list, icao_list, icaos_aircraft, iatas_aircraft, cutoff=0.65):\n",
    "    print(f\"Cutoff: {cutoff}\\n\\n\")\n",
    "    dfs = [(key, dbs_dct[key]['FullName'].to_frame()) for key in dbs_dct if key not in skip_list]\n",
    "    \n",
    "    analysis = analyze(main_frame, dfs[:], left_on, right_on, cutoff=cutoff)\n",
    "    df = analysis.copy()\n",
    "\n",
    "    derive_best_score(df)\n",
    "\n",
    "    df = get_info_best(df, 'best_cutoff')\n",
    "\n",
    "    df['orig_database'] = find_db(df, 'codes_base_num', dfs)\n",
    "\n",
    "    df['have_icao'] = desig_availability(df, 'orig_database', icaos_aircraft)\n",
    "    df['have_iata'] = desig_availability(df, 'orig_database', iatas_aircraft)\n",
    "    df['ICAO'] = get_desig_values(df, 'orig_database', 'have_icao', dbs_dct, 'ICAO')\n",
    "    df['IATA'] = get_desig_values(df, 'orig_database', 'have_iata', dbs_dct, 'IATA')\n",
    "    df['researched_value'] = get_values(df, 'best_match')\n",
    "    df['cutoff'] = get_values(df, 'best_cutoff')\n",
    "    \n",
    "    analysis_rematch = rematching(df, dbs_dct, left_on, right_on, skip_list, icao_list, icaos_aircraft, iatas_aircraft)\n",
    "    \n",
    "    for row in analysis_rematch.index:\n",
    "        df.loc[row, 'orig_database'] = analysis_rematch.loc[row, 'orig_database']\n",
    "        df.loc[row, 'ICAO'] = analysis_rematch.loc[row, 'ICAO']\n",
    "        df.loc[row, 'IATA'] = analysis_rematch.loc[row, 'IATA']\n",
    "        df.loc[row, 'researched_value'] = analysis_rematch.loc[row, 'researched_value']\n",
    "        df.loc[row, 'cutoff'] = analysis_rematch.loc[row, 'cutoff']\n",
    "    \n",
    "    for row in df.index:\n",
    "    \n",
    "        dataset = df.loc[row, 'orig_database']\n",
    "        \n",
    "        if not pd.isnull(dataset):\n",
    "\n",
    "            df.loc[row, 'orig_database'] = database_url[dataset]\n",
    "\n",
    "        else:\n",
    "\n",
    "            pass\n",
    "        \n",
    "    df = df[[left_on, 'best_match', 'best_match_index', 'codes_base_num',\n",
    "             'have_icao', 'have_iata', 'ICAO', 'IATA', 'researched_value',\n",
    "             'cutoff', 'orig_database']]\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5ebaed",
   "metadata": {},
   "source": [
    "## Add info on received ICAO and IATA designators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_info(df, dbs_dct):\n",
    "    fan_cols = ['Manufacturer', 'FullName', 'ICAO', 'Manufacturer country', 'Category', 'Role']\n",
    "    keep_cols = ['Aircraft ICAO', 'Aircraft IATA', 'ManufacturerCode', 'Description', \n",
    "                 'AircraftDescription', 'EngineCount', 'EngineType', 'WTC', 'WTG', \n",
    "                 'Role', 'researched_aircraft_value', 'aircraft_cutoff', 'aircrafts_database']\n",
    "    df_merged = pd.merge(df, dbs_dct['icao'].groupby(by='ICAO').first() \\\n",
    "                         .reset_index(), left_on='Aircraft ICAO', right_on='ICAO', how='left', suffixes=(None, '_check'))\n",
    "    \n",
    "    df_merged1 = pd.merge(df_merged, dbs_dct['iata'].groupby(by='IATA').first() \\\n",
    "                          .reset_index(), left_on='Aircraft IATA', right_on='IATA', how='left', suffixes=(None, '_check'))\n",
    "    \n",
    "    df_merged2 = pd.merge(df_merged1, dbs_dct['fan'][fan_cols].groupby(by='ICAO').first() \\\n",
    "                          .reset_index(), left_on='Aircraft ICAO', right_on='ICAO', how='left')\n",
    "    \n",
    "    df_merged2 = df_merged2[keep_cols]\n",
    "    return df_merged2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ed57d",
   "metadata": {},
   "source": [
    "## Pretty printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendix(db, db_urls, status):\n",
    "    \"\"\"\n",
    "    Pretty print some specified load status information of datasets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    db: str\n",
    "        Name of dataset\n",
    "    \n",
    "    db_urls: dict\n",
    "        Dictionary, containing urls of all datasets\n",
    "        \n",
    "    status: bool\n",
    "        Status of dataset load\n",
    "    \"\"\"\n",
    "    appendix1 = ' ' if status else ' not '\n",
    "    appendix2 = 'loaded from the disk. (reload dataset from time to time to update data)' if status else 'parsed that would take some time.'\n",
    "    return db_urls[db], appendix1, appendix2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c2aaf",
   "metadata": {},
   "source": [
    "## Creation of the final report files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report(overall, matches_full, overall_exp, corrupted, index_visualize=True) -> None:\n",
    "    \"\"\"\n",
    "    Create Excel report about aviacompanies' fleet \n",
    "    with making directory and specifying date and time\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    overall: pd.DataFrame\n",
    "        DataFrame with information \n",
    "        about aviacompanies' fleet\n",
    "        \n",
    "    matches_airlines: pd.DataFrame\n",
    "        DataFrame of airlines matches\n",
    "        \n",
    "    matches_full: pd.DataFrame\n",
    "        DataFrame of aircrafts matches with added information\n",
    "        \n",
    "    overal_exp: pd.DataFrame\n",
    "        DataFrame with information \n",
    "        about parsed wiki pages exceptions \n",
    "        of aviacompanies' fleet\n",
    "        \n",
    "    corrupted: list\n",
    "        List of all corrupted URLs\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    t_date = datetime.now().strftime('%d_%m_%Y, %H.%M')\n",
    "\n",
    "    Path(f\"{path}/report_{t_date}\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    columns_order = [\n",
    "        'Company', 'Airline ICAO', 'Airline IATA', 'Callsign', 'Wiki URL', 'Aircraft',\n",
    "        'In service', 'Orders', 'Total_Passengers', 'Notes','Aircraft ICAO',\n",
    "        'Aircraft IATA', 'ManufacturerCode', 'Description', 'AircraftDescription', \n",
    "        'EngineCount', 'EngineType', 'WTC', 'WTG', 'Role', 'researched_aircraft_value', \n",
    "        'aircraft_cutoff','aircrafts_database'\n",
    "    ]\n",
    "    \n",
    "    os.chdir(f\"{path}/report_{t_date}\")\n",
    "    if index_visualize:\n",
    "        multiindex_cols = ['Company', 'Airline ICAO', 'Airline IATA', 'Callsign', 'Wiki URL', 'Aircraft', 'In service', 'Orders']\n",
    "    \n",
    "    if corrupted: \n",
    "        pd.DataFrame(corrupted, columns=['Corrupted URLs']).to_excel(f'corrupted_URLs_{t_date}.xlsx', index=False)\n",
    "    \n",
    "    if overall is not None:\n",
    "        final_table = pd.concat([overall.reset_index(drop=True), matches_full], axis=1)[columns_order]\n",
    "        \n",
    "        if index_visualize:\n",
    "            final_table.set_index(multiindex_cols, inplace=True)\n",
    "            \n",
    "        final_table.to_excel(f'fleet_report_{t_date}.xlsx', index=False)\n",
    "        \n",
    "    if overall_exp is not None: \n",
    "        overall_exp.to_excel(f'fleet_report_exceptions_{t_date}.xlsx')\n",
    "    \n",
    "    print(f'Reports were created and stored at {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788d57ea",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c638052",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a4a74",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a23f31",
   "metadata": {},
   "source": [
    "<!-- --------- -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f324ebd",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6badb0",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3a69b",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76220f5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:100px\">Parser</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39462da",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_started = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5e60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35f39e",
   "metadata": {},
   "source": [
    "### 1. Setting up of initial data and getting parse urls\n",
    "1. In a variable **source** write down what source to parse objects from\n",
    "\n",
    "    possible options: **{'pulkovo', 'us', 'europe', 'cac', 'africa', 'asia', 'northamerica', 'oceania', 'southamerica', 'all', 'test', 'national', 'pulkovo_preloaded'}**\n",
    "\n",
    "\n",
    "2. In a variable **mode**  write what type of data to gather\n",
    "\n",
    "    possible options: **{'urls', 'names'}**\n",
    "\n",
    "NOTE! You can use only 'names' option in a variable **mode** if the source==**'pulkovo'**\n",
    "\n",
    "NOTE! You can use source==**'pulkovo_preloaded'** only if you have appropriate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4234b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'test'\n",
    "mode = 'urls'\n",
    "parse_obj = get_list_companies(source=source, mode=mode, headers=headers)\n",
    "urls = get_parse_urls(parse_obj=parse_obj, mode=mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31710a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "responses, corrupted_ = get_responses(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68120666",
   "metadata": {},
   "source": [
    "#### List of unresponded corrupted URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312f453",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afa254",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([i.status_code if i is not None else '' for i in corrupted_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0624d",
   "metadata": {},
   "source": [
    "### 2. Main fleet parser (uses urls variable to iterate and parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd72894f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "fleet = []\n",
    "corrupted = []\n",
    "exceptions = []\n",
    "\n",
    "for num, response in enumerate(responses[:]):\n",
    "\n",
    "    df = []\n",
    "    company = get_company_name(response)\n",
    "    \n",
    "    df, got = get_table(response, headers)\n",
    "\n",
    "    if not got:\n",
    "        \n",
    "        df, got = get_table(response, headers, extra_page=True)\n",
    "            \n",
    "    if not got:\n",
    "        \n",
    "        df1, got = get_no_table(response, company, headers)\n",
    "        \n",
    "        if got == 'table':\n",
    "            fleet.append(df1)   \n",
    "            logging(company, response, responses, num, status='Non-table transformed to table and parsed')\n",
    "            continue\n",
    "        \n",
    "        if got == 'text':\n",
    "            logging(company, response, responses, num, status='Non-table text exception')\n",
    "            exceptions.append(df1)\n",
    "            continue\n",
    "   \n",
    "    if not got:\n",
    "        logging(company, response, responses, num, status='Corrupted')\n",
    "        corrupted.append(response.url)\n",
    "        continue\n",
    "    \n",
    "    df1 = exception_handler(df)\n",
    "    \n",
    "    df1 = transform_table(df1, company, response)\n",
    "        \n",
    "    fleet.append(df1)   \n",
    "    logging(company, response, responses, num, status='Table parsed')\n",
    "    \n",
    "success = len(fleet)+len(exceptions)\n",
    "print('', '', '', sep='\\n')\n",
    "print(f'{success}/{len(responses)} companies successfully parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9922fbbc",
   "metadata": {},
   "source": [
    "#### Statistics for parsed urls\n",
    "* len(fleet) -- stands for the number of processed tables with fleet information\n",
    "* len(exception) -- stands for the number of received non-table fleet info that for the moment could not be processed further\n",
    "* len(corrupted) -- stands for the number of URLs that could not be parsed due to the different reasons\n",
    "* len(corrupted_) -- stands for the number of URLs that could not be parsed due to the absence of page on website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516673e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(fleet), len(exceptions), len(corrupted), len(corrupted_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e679f4a",
   "metadata": {},
   "source": [
    "#### List of corrupted URLs during the processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc821c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c522985e",
   "metadata": {},
   "source": [
    "### 3. Concatenation of received parsed tables with fleet info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall, overall_exp = concat_tables(fleet, exceptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc43cc",
   "metadata": {},
   "source": [
    "### 4. Setting up the storage and databases paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be401771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage path\n",
    "\n",
    "path = \"C:\\\\Users\\\\Asus\\\\Downloads\\\\parser\\\\reports\"\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to databases\n",
    "\n",
    "db_path = \"datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.copy(f\"{db_path}fan_codes.xlsx\", f\"{path}\")\n",
    "    preloaded_fan=True\n",
    "except:\n",
    "    preloaded_fan=False\n",
    "to_print = appendix('fan', database_url, preloaded_fan)\n",
    "print(f'Heavy dataset {to_print[0]} is{to_print[1]}preloaded.\\nIt is going to be {to_print[2]}\\n')\n",
    "\n",
    "try:    \n",
    "    shutil.copy(f\"{db_path}mapper_codes.xlsx\", f\"{path}\")\n",
    "    preloaded_map=True\n",
    "except:\n",
    "    preloaded_map=False   \n",
    "to_print = appendix('mapper', database_url, preloaded_map)\n",
    "print(f'Heavy dataset {to_print[0]} is{to_print[1]}preloaded.\\nIt is going to be {to_print[2]}\\n')\n",
    "    \n",
    "try:    \n",
    "    shutil.copy(f\"{db_path}av_airlines.xlsx\", f\"{path}\")\n",
    "    preloaded_av=True\n",
    "except:\n",
    "    preloaded_av=False    \n",
    "to_print = appendix('av_airlines', database_url, preloaded_av)\n",
    "print(f'Heavy dataset {to_print[0]} is{to_print[1]}preloaded.\\nIt is going to be {to_print[2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566bbe9",
   "metadata": {},
   "source": [
    "### 5. Loading and processing external databases\n",
    "####  Any of realisations might be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# all_codes = load_databases()\n",
    "all_codes_airlines = {\n",
    "                      'code': load_wiki4(), \n",
    "                      'av_airlines': load_av_airlines(preloaded=preloaded_av),\n",
    "                      'flug_airlines': load_flug_airlines(),\n",
    "                      'aircodes': load_aircodes()\n",
    "                    }\n",
    "\n",
    "print('\\nAirlines datasets were loaded.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f59a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_codes = {\n",
    "         'iata': load_ssim(), 'icao': load_icao(), 'wiki1': load_wiki1(), \n",
    "         'wiki2': load_wiki2(), 'wiki3': load_wiki3(), 'fan': load_fan(preloaded=preloaded_fan), \n",
    "         'mapper': load_mapper(preloaded=preloaded_map), 'military': load_mil(), 'flug_iata': load_flug_iata(),\n",
    "         'flug_icao': load_flug_icao(), 'av': load_av()\n",
    "        }\n",
    "\n",
    "print('\\nAircrafts datasets were loaded.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e464870",
   "metadata": {},
   "source": [
    "### 6. Derivation and processing the column on which the match will be searched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d98bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_frame_aircrafts = derive_match_col(overall, col_to_derive='Aircraft', change_values_of=change_values_of_aircrafts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a704089",
   "metadata": {},
   "source": [
    "### 7. Advanced fuzzy matching of ICAO and IATA codes to the aircrafts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39892c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matches_aircrafts = matching(main_frame_aircrafts, all_codes, 'Crafts', 'FullName', skip_list, icao_aircraft_lst, icaos_aircraft, iatas_aircraft)\n",
    "matches_aircrafts = matches_aircrafts[['ICAO', 'IATA', 'researched_value', 'cutoff','orig_database']]\n",
    "matches_aircrafts = matches_aircrafts.rename(columns={'ICAO':'Aircraft ICAO', 'IATA':'Aircraft IATA',\n",
    "                                                      'researched_value':'researched_aircraft_value',\n",
    "                                                      'cutoff':'aircraft_cutoff', 'orig_database':'aircrafts_database'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c159cba0",
   "metadata": {},
   "source": [
    "### 8. Adding the info to the matched ICAO and IATA designators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12713c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_full = add_info(matches_aircrafts, all_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e2c2a",
   "metadata": {},
   "source": [
    "### 9. Creation of final report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76f020",
   "metadata": {},
   "source": [
    "If it is needed to unite information about airlines for visual understanding, set parameter **index_visualize** to True in **create_report()** function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b84cc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_report(overall, matches_full, overall_exp, corrupted+corrupted_, index_visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a68ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Total running time: {(datetime.now()-script_started)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81917723",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
